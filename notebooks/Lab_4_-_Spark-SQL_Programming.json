{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 4 - Spark-SQL Programming**\n\nSo far, we have used Zeppelin notebooks to examine Spark-SQL capabilities.  We'll finish this workshop by creating a standalone Java program that uses Spark-SQL.\n\nOur standalone program will recreate some of the work we did in Lab 3.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement an ETL process that does the following:\n\n- **import transaction data** that originated from a business partner","dateUpdated":"2019-06-23T13:06:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127953_-1712440608","id":"20190615-193334_2123319074","dateCreated":"2019-06-22T20:25:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2177","user":"anonymous","dateFinished":"2019-06-23T13:06:55+0000","dateStarted":"2019-06-23T13:06:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 4 - Spark-SQL Programming</strong></h1>\n<p>So far, we have used Zeppelin notebooks to examine Spark-SQL capabilities. We&rsquo;ll finish this workshop by creating a standalone Java program that uses Spark-SQL.</p>\n<p>Our standalone program will recreate some of the work we did in Lab 3.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement an ETL process that does the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n</ul>\n</div>"}]}},{"text":"%spark\n\n// cell 2\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 AND/OR LAB3 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-22T23:29:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127954_-1711286361","id":"20190615-195433_1606768454","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:29:04+0000","dateFinished":"2019-06-22T23:29:29+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2178"},{"text":"%cassandra\n\n// cell 3\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-22T23:29:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127955_-1711671110","id":"20190620-185951_1579318164","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:29:55+0000","dateFinished":"2019-06-22T23:29:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2179"},{"text":"%md\n\ncell 4\n\n### Simulating our program...\n\nThe cell below gathers up the Lab 3 code we ran in multiple cells.  \n\nIt reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.\n","dateUpdated":"2019-06-23T13:07:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127956_-1713594855","id":"20190620-185404_1847570742","dateCreated":"2019-06-22T20:25:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2180","user":"anonymous","dateFinished":"2019-06-23T13:07:00+0000","dateStarted":"2019-06-23T13:07:00+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 4</p>\n<h3>Simulating our program&hellip;</h3>\n<p>The cell below gathers up the Lab 3 code we ran in multiple cells. </p>\n<p>It reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 5\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");\n\n// Now let's turn it into a View so we can use SQL against it.\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n// IT ALL ACTS AS A SINGLE DATABASE!!!\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\") ","user":"anonymous","dateUpdated":"2019-06-22T23:34:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127956_-1713594855","id":"20190615-184855_149703656","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:34:08+0000","dateFinished":"2019-06-22T23:34:21+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2181"},{"text":"%spark\n\n// cell 6\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-22T23:34:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127957_-1713979604","id":"20190620-201216_1498917214","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:34:26+0000","dateFinished":"2019-06-22T23:34:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2182"},{"text":"%cassandra\n\n// cell 7\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-22T23:46:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127958_-1712825357","id":"20190620-202419_1221985005","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:46:38+0000","dateFinished":"2019-06-22T23:46:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2183"},{"text":"%cassandra\n\n// cell 8\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-22T23:46:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127958_-1712825357","id":"20190620-202635_2085496124","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:46:48+0000","dateFinished":"2019-06-22T23:46:48+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2184"},{"text":"%md\n\ncell 9\n\n### Examining the code\n\nLet's see how all of the above looks in a standalone Java program.\n\n```\npackage com.datastax.kellett;\n\nimport java.util.Scanner;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\npublic class Lab4 {\n    public static void main(String[] args) {\n\n        // Connect to Spark\n        System.out.println(\"About to build session...\");\n        SparkSession spark = SparkSession.builder()\n                .appName(\"Spark-SQL Lab 4\")           // any name you like... will display in Spark Management UI\n                .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate(); // gets existing spark session if present, or creates a new one\n\n        // We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n        // In the real world, this could be a flat file or perhaps a database table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(\"About to read transactions_buy.csv...\");\n        Dataset<Row> incoming_buy_transactions = spark.read().option(\"header\", true).csv(\"analytics_workshop.transactions_buy.csv\");\n\n        // Now let's turn it into a View so we can use SQL against it.\n        incoming_buy_transactions.createOrReplaceTempView(\"new_buys\");\n\n        // Next, let's whittle the table down so it only shows large transactions.\n        // Notice that we are now working in pure SQL.\n        Dataset<Row> newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\");\n\n        // The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(\"About to read transactions_sell.csv...\");\n        Dataset<Row> incoming_sell_transactions = spark.read().option(\"header\", true).csv(\"analytics_workshop.transactions_sell.csv\");\n        incoming_sell_transactions.createOrReplaceTempView(\"new_sells\");\n        Dataset<Row> newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\");\n\n        // Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table,\n        // and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n        // and then inserts the entire result into the (Cassandra) transactions_historical table.\n        // IT ALL ACTS AS A SINGLE DATABASE!!!\n        System.out.println(\"About to perform SQL INSERT operation...\");\n        Dataset<Row> ds = spark.sql(\"INSERT INTO analytics_workshop.transactions_historical \"\n                + \"SELECT \"\n                + \"c.account_number        AS account_number, \"\n                + \"t.transaction_id        AS transaction_id, \"\n                + \"c.city                  AS account_city, \"\n                + \"c.country               AS account_country, \"\n                + \"c.first_name            AS account_first_name, \"\n                + \"c.last_name             AS account_last_name, \"\n                + \"c.gender                AS account_gender, \"\n                + \"t.buy_or_sell           AS buy_or_sell, \"\n                + \"t.industry_sector       AS industry_sector, \"\n                + \"t.instrument_id         AS instrument_id, \"\n                + \"t.instrument_industry   AS instrument_industry, \"\n                + \"t.instrument_name       AS instrument_name, \"\n                + \"t.transaction_date      AS transaction_date, \"\n                + \"t.transaction_time      AS transaction_time, \"\n                + \"t.units                 AS units \"\n\n                + \"FROM new_buys t INNER JOIN analytics_workshop.customers c \"\n                + \"ON t.account_number = c.account_number \"\n                + \"WHERE t.units > 999 \"\n\n                + \"UNION \"\n\n                + \"SELECT \"\n                + \"c.account_number        AS account_number, \"\n                + \"t.transaction_id        AS transaction_id, \"\n                + \"c.city                  AS account_city, \"\n                + \"c.country               AS account_country, \"\n                + \"c.first_name            AS account_first_name, \"\n                + \"c.last_name             AS account_last_name, \"\n                + \"c.gender                AS account_gender, \"\n                + \"t.buy_or_sell           AS buy_or_sell, \"\n                + \"t.industry_sector       AS industry_sector, \"\n                + \"t.instrument_id         AS instrument_id, \"\n                + \"t.instrument_industry   AS instrument_industry, \"\n                + \"t.instrument_name       AS instrument_name, \"\n                + \"t.transaction_date      AS transaction_date, \"\n                + \"t.transaction_time      AS transaction_time, \"\n                + \"t.units                 AS units \"\n\n                + \"FROM new_sells t INNER JOIN analytics_workshop.customers c \"\n                + \"ON t.account_number = c.account_number \"\n                + \"WHERE t.units > 999\");\n\n        System.out.println(\"... SQL INSERT operation complete.\");\n\n        System.out.println(\"Waiting here to allow time for viewing Spark UI on port 7080.\");\n        System.out.println(\"Press enter when you want to terminate.\");\n        Scanner scanner = new Scanner(System.in);\n        scanner.nextLine();\n\n        spark.close();\n    }\n}\n\n```\n","dateUpdated":"2019-06-23T13:07:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127959_-1713210106","id":"20190619-210148_86481232","dateCreated":"2019-06-22T20:25:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2185","user":"anonymous","dateFinished":"2019-06-23T13:07:08+0000","dateStarted":"2019-06-23T13:07:08+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 9</p>\n<h3>Examining the code</h3>\n<p>Let&rsquo;s see how all of the above looks in a standalone Java program.</p>\n<pre><code>package com.datastax.kellett;\n\nimport java.util.Scanner;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\npublic class Lab4 {\n    public static void main(String[] args) {\n\n        // Connect to Spark\n        System.out.println(&quot;About to build session...&quot;);\n        SparkSession spark = SparkSession.builder()\n                .appName(&quot;Spark-SQL Lab 4&quot;)           // any name you like... will display in Spark Management UI\n                .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate(); // gets existing spark session if present, or creates a new one\n\n        // We&#39;ll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n        // In the real world, this could be a flat file or perhaps a database table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(&quot;About to read transactions_buy.csv...&quot;);\n        Dataset&lt;Row&gt; incoming_buy_transactions = spark.read().option(&quot;header&quot;, true).csv(&quot;analytics_workshop.transactions_buy.csv&quot;);\n\n        // Now let&#39;s turn it into a View so we can use SQL against it.\n        incoming_buy_transactions.createOrReplaceTempView(&quot;new_buys&quot;);\n\n        // Next, let&#39;s whittle the table down so it only shows large transactions.\n        // Notice that we are now working in pure SQL.\n        Dataset&lt;Row&gt; newBuys = spark.sql(&quot;SELECT * FROM new_buys WHERE units &gt; 999&quot;);\n\n        // The new_buys table looks good.  Now let&#39;s do all the same things and make a new_sells table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(&quot;About to read transactions_sell.csv...&quot;);\n        Dataset&lt;Row&gt; incoming_sell_transactions = spark.read().option(&quot;header&quot;, true).csv(&quot;analytics_workshop.transactions_sell.csv&quot;);\n        incoming_sell_transactions.createOrReplaceTempView(&quot;new_sells&quot;);\n        Dataset&lt;Row&gt; newSells = spark.sql(&quot;SELECT * FROM new_sells WHERE units &gt; 999&quot;);\n\n        // Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table,\n        // and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n        // and then inserts the entire result into the (Cassandra) transactions_historical table.\n        // IT ALL ACTS AS A SINGLE DATABASE!!!\n        System.out.println(&quot;About to perform SQL INSERT operation...&quot;);\n        Dataset&lt;Row&gt; ds = spark.sql(&quot;INSERT INTO analytics_workshop.transactions_historical &quot;\n                + &quot;SELECT &quot;\n                + &quot;c.account_number        AS account_number, &quot;\n                + &quot;t.transaction_id        AS transaction_id, &quot;\n                + &quot;c.city                  AS account_city, &quot;\n                + &quot;c.country               AS account_country, &quot;\n                + &quot;c.first_name            AS account_first_name, &quot;\n                + &quot;c.last_name             AS account_last_name, &quot;\n                + &quot;c.gender                AS account_gender, &quot;\n                + &quot;t.buy_or_sell           AS buy_or_sell, &quot;\n                + &quot;t.industry_sector       AS industry_sector, &quot;\n                + &quot;t.instrument_id         AS instrument_id, &quot;\n                + &quot;t.instrument_industry   AS instrument_industry, &quot;\n                + &quot;t.instrument_name       AS instrument_name, &quot;\n                + &quot;t.transaction_date      AS transaction_date, &quot;\n                + &quot;t.transaction_time      AS transaction_time, &quot;\n                + &quot;t.units                 AS units &quot;\n\n                + &quot;FROM new_buys t INNER JOIN analytics_workshop.customers c &quot;\n                + &quot;ON t.account_number = c.account_number &quot;\n                + &quot;WHERE t.units &gt; 999 &quot;\n\n                + &quot;UNION &quot;\n\n                + &quot;SELECT &quot;\n                + &quot;c.account_number        AS account_number, &quot;\n                + &quot;t.transaction_id        AS transaction_id, &quot;\n                + &quot;c.city                  AS account_city, &quot;\n                + &quot;c.country               AS account_country, &quot;\n                + &quot;c.first_name            AS account_first_name, &quot;\n                + &quot;c.last_name             AS account_last_name, &quot;\n                + &quot;c.gender                AS account_gender, &quot;\n                + &quot;t.buy_or_sell           AS buy_or_sell, &quot;\n                + &quot;t.industry_sector       AS industry_sector, &quot;\n                + &quot;t.instrument_id         AS instrument_id, &quot;\n                + &quot;t.instrument_industry   AS instrument_industry, &quot;\n                + &quot;t.instrument_name       AS instrument_name, &quot;\n                + &quot;t.transaction_date      AS transaction_date, &quot;\n                + &quot;t.transaction_time      AS transaction_time, &quot;\n                + &quot;t.units                 AS units &quot;\n\n                + &quot;FROM new_sells t INNER JOIN analytics_workshop.customers c &quot;\n                + &quot;ON t.account_number = c.account_number &quot;\n                + &quot;WHERE t.units &gt; 999&quot;);\n\n        System.out.println(&quot;... SQL INSERT operation complete.&quot;);\n\n        System.out.println(&quot;Waiting here to allow time for viewing Spark UI on port 7080.&quot;);\n        System.out.println(&quot;Press enter when you want to terminate.&quot;);\n        Scanner scanner = new Scanner(System.in);\n        scanner.nextLine();\n\n        spark.close();\n    }\n}\n\n</code></pre>\n</div>"}]}},{"text":"%md\n\ncell 10\n\n### Examining the build\n\nLet's examine the maven .pom file used to build the application.\n\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n\t<modelVersion>4.0.0</modelVersion>\n\t<url>http://maven.apache.org</url>\n\t\n    <!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-sql-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the <repositories> section (if needed) and the <properties> section (if needed)\n\t\tand the <build> section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    -->\t\n\t\n\t<groupId>com.datastax.kellett</groupId>\n\t<artifactId>datastax-spark-sql-v2</artifactId>\n\t<version>0.0.1-SNAPSHOT</version>\n\t<packaging>jar</packaging>\n\t<name>datastax-spark-sql-v2</name>\n\n\t<properties> <!-- Not currently used, but we keep them as doc and for possible future use -->\n\t\t<java.version>1.8</java.version>\n\t\t<dse.version>6.0.7</dse.version>\n        <scala.version>2.11.8</scala.version>\n\t</properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId> <!-- 2.11 is Scala version used to build Spark -->\n            <version>2.3.2</version>  \t\t\t\t<!-- this number refers to the Spark version -->\n        </dependency>  <!--see https://spark.apache.org/downloads.html to get latest info for both of the above -->\n\t</dependencies>\n\n\n    <build>\n        <finalName>datastax-spark-sql-v2</finalName>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <executions>\n                    <execution>\n                        <id>create-my-bundle</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                        <configuration>\n                            <descriptorRefs>\n                                <descriptorRef>jar-with-dependencies</descriptorRef>\n                            </descriptorRefs>\n                        </configuration>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n    <repositories>\n        <repository>\n            <id>DataStax-Repo</id>\n            <url>https://datastax.artifactoryonline.com/datastax/public-repos/</url>\n        </repository>\n    </repositories>\n\n</project>\n\n```","user":"anonymous","dateUpdated":"2019-06-23T13:07:17+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127960_-1715133850","id":"20190621-191743_2003951309","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-23T13:07:17+0000","dateFinished":"2019-06-23T13:07:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2186","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Examining the build</h3>\n<p>Let&rsquo;s examine the maven .pom file used to build the application.</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\n\txsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n\t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n\t&lt;url&gt;http://maven.apache.org&lt;/url&gt;\n\t\n    &lt;!-- above is all boilerplate\n        \n\t\tNOTE that I generated my base pom and project directory structure with the following maven command (run from \n\t\tthe directory ABOVE where you want the project to be created... e.g. /maven):\n        mvn archetype:generate -DgroupId=com.datastax.kellett -DartifactId=datastax-spark-sql-v2 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false\n\n        The junit dependency was created automatically for me, but I pasted in the other dependencies (if needed)\n        and the &lt;repositories&gt; section (if needed) and the &lt;properties&gt; section (if needed)\n\t\tand the &lt;build&gt; section all BEFORE importing the project into IntelliJ.  \n\t\tI imported by pointing to the datastax-spark-sql-v2 directory, and imported as Maven project.\n    --&gt;\t\n\t\n\t&lt;groupId&gt;com.datastax.kellett&lt;/groupId&gt;\n\t&lt;artifactId&gt;datastax-spark-sql-v2&lt;/artifactId&gt;\n\t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n\t&lt;packaging&gt;jar&lt;/packaging&gt;\n\t&lt;name&gt;datastax-spark-sql-v2&lt;/name&gt;\n\n\t&lt;properties&gt; &lt;!-- Not currently used, but we keep them as doc and for possible future use --&gt;\n\t\t&lt;java.version&gt;1.8&lt;/java.version&gt;\n\t\t&lt;dse.version&gt;6.0.7&lt;/dse.version&gt;\n        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;\n\t&lt;/properties&gt;\n\n    &lt;dependencies&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;junit&lt;/groupId&gt;\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\n            &lt;version&gt;3.8.1&lt;/version&gt;\n            &lt;scope&gt;test&lt;/scope&gt;\n        &lt;/dependency&gt;\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;!-- 2.11 is Scala version used to build Spark --&gt;\n            &lt;version&gt;2.3.2&lt;/version&gt;  \t\t\t\t&lt;!-- this number refers to the Spark version --&gt;\n        &lt;/dependency&gt;  &lt;!--see https://spark.apache.org/downloads.html to get latest info for both of the above --&gt;\n\t&lt;/dependencies&gt;\n\n\n    &lt;build&gt;\n        &lt;finalName&gt;datastax-spark-sql-v2&lt;/finalName&gt;\n        &lt;plugins&gt;\n            &lt;plugin&gt;\n                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\n                &lt;executions&gt;\n                    &lt;execution&gt;\n                        &lt;id&gt;create-my-bundle&lt;/id&gt;\n                        &lt;phase&gt;package&lt;/phase&gt;\n                        &lt;goals&gt;\n                            &lt;goal&gt;single&lt;/goal&gt;\n                        &lt;/goals&gt;\n                        &lt;configuration&gt;\n                            &lt;descriptorRefs&gt;\n                                &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;\n                            &lt;/descriptorRefs&gt;\n                        &lt;/configuration&gt;\n                    &lt;/execution&gt;\n                &lt;/executions&gt;\n            &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n\n    &lt;repositories&gt;\n        &lt;repository&gt;\n            &lt;id&gt;DataStax-Repo&lt;/id&gt;\n            &lt;url&gt;https://datastax.artifactoryonline.com/datastax/public-repos/&lt;/url&gt;\n        &lt;/repository&gt;\n    &lt;/repositories&gt;\n\n&lt;/project&gt;\n\n</code></pre>\n</div>"}]}},{"text":"%md\n\ncell 11\n\n### Running the code\n\nLet's submit our Spark job and see if it works.","dateUpdated":"2019-06-23T13:07:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127961_-1715518599","id":"20190621-124749_1514972164","dateCreated":"2019-06-22T20:25:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2187","user":"anonymous","dateFinished":"2019-06-23T13:07:26+0000","dateStarted":"2019-06-23T13:07:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 11</p>\n<h3>Running the code</h3>\n<p>Let&rsquo;s submit our Spark job and see if it works.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 12\n\n// Before we run the Spark job we'll truncate our destination table so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-22T23:46:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127961_-1715518599","id":"20190620-220743_317732750","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:46:59+0000","dateFinished":"2019-06-22T23:46:59+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2188"},{"text":"%cassandra\n\n// cell 13\n\n// Verify that the table has no rows\n\nselect count(*) from analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-22T23:47:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127962_-1714364352","id":"20190621-175216_909909353","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-22T23:47:03+0000","dateFinished":"2019-06-22T23:47:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2189"},{"text":"%md\n\ncell 14\n\nNote that we have placed the input flat files on the dsefs file system so they are available to all nodes in the cluster.\n\nYou can verify this with the following terminal commands:\n\n```\ndse fs \n\nls\n\nexit\n```","user":"anonymous","dateUpdated":"2019-06-23T13:07:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127964_-1716672846","id":"20190621-180218_1343810484","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-23T13:07:35+0000","dateFinished":"2019-06-23T13:07:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2190","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 14</p>\n<p>Note that we have placed the input flat files on the dsefs file system so they are available to all nodes in the cluster.</p>\n<p>You can verify this with the following terminal commands:</p>\n<pre><code>dse fs \n\nls\n\nexit\n</code></pre>\n</div>"}]}},{"text":"%md\n\ncell 15\n\nNow we can submit the spark job via the dse utility. \n\nCopy the following onto a terminal window:\n\n```\ncd /tmp/datastax-spark-sql-workshop/executables\n```\n\n```\ndse spark-submit --class com.datastax.kellett.Lab4 --total-executor-cores 6 --executor-memory 1g datastax-spark-sql-v2-jar-with-dependencies.jar\n```","user":"anonymous","dateUpdated":"2019-06-23T13:07:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127963_-1714749101","id":"20190621-184507_734246512","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-23T13:07:45+0000","dateFinished":"2019-06-23T13:07:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2191","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 15</p>\n<p>Now we can submit the spark job via the dse utility. </p>\n<p>Copy the following onto a terminal window:</p>\n<pre><code>cd /tmp/datastax-spark-sql-workshop/executables\n</code></pre>\n<pre><code>dse spark-submit --class com.datastax.kellett.Lab4 --total-executor-cores 6 --executor-memory 1g datastax-spark-sql-v2-jar-with-dependencies.jar\n</code></pre>\n</div>"}]}},{"text":"%md\n\ncell 16\n\n### Summary\n\nWe have seen how to create a complete Spark-SQL application in Java.\n","user":"anonymous","dateUpdated":"2019-06-23T13:07:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127964_-1716672846","id":"20190620-221624_2092246902","dateCreated":"2019-06-22T20:25:27+0000","dateStarted":"2019-06-23T13:07:57+0000","dateFinished":"2019-06-23T13:07:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2192","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 16</p>\n<h3>Summary</h3>\n<p>We have seen how to create a complete Spark-SQL application in Java.</p>\n</div>"}]}},{"text":"","user":"anonymous","dateUpdated":"2019-06-23T13:02:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561235127965_-1717057595","id":"20190620-223813_1558396189","dateCreated":"2019-06-22T20:25:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2193"}],"name":"Lab_4_-_Spark-SQL_Programming","id":"2EFVF2F7A","angularObjects":{"2EGNSJ43D:shared_process":[],"2EDF8BEAT:shared_process":[],"2EDZ95MW4:shared_process":[],"2EE3AEHFV:shared_process":[],"2EERYZDAF:shared_process":[],"2EG7QTSMG:shared_process":[],"2EGE2N4X9:shared_process":[],"2EE9V8B1K:shared_process":[],"2EDGUFYW4:shared_process":[],"2EFN7C5HN:shared_process":[],"2EEX3QUX9:shared_process":[],"2ED6HEMRW:shared_process":[],"2EFWFFXBN:shared_process":[],"2EEWYZZEJ:shared_process":[],"2EFVCFB3K:shared_process":[],"2EESDPBRQ:shared_process":[],"2EFBTV85N:shared_process":[],"2EFP8GTK2:shared_process":[],"2EEG3YR46:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}