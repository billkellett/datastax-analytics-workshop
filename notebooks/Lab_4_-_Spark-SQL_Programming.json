{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 4 - Spark-SQL Programming**\n\nSo far, we have used Zeppelin notebooks to examine Spark-SQL capabilities.  We'll finish this workshop by creating a standalone Java program that uses Spark-SQL.\n\nOur standalone program will recreate some of the work we did in Lab 3.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement an ETL process that does the following:\n\n- **import transaction data** that originated from a business partner","user":"anonymous","dateUpdated":"2019-06-21T12:13:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 4 - Spark-SQL Programming</strong></h1>\n<p>So far, we have used Zeppelin notebooks to examine Spark-SQL capabilities. We&rsquo;ll finish this workshop by creating a standalone Java program that uses Spark-SQL.</p>\n<p>Our standalone program will recreate some of the work we did in Lab 3.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement an ETL process that does the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017724_1084305698","id":"20190615-193334_2123319074","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:13:39+0000","dateFinished":"2019-06-21T12:13:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3701"},{"text":"%spark\n\n// cell 2\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 AND/OR LAB3 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-21T19:38:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017746_1175875936","id":"20190615-195433_1606768454","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:53:57+0000","dateFinished":"2019-06-21T12:54:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3702"},{"text":"%cassandra\n\n// cell 3\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-21T12:54:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017749_1173182694","id":"20190620-185951_1579318164","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:54:08+0000","dateFinished":"2019-06-21T12:54:16+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3703"},{"text":"%md\n\ncell 4\n\n### Simulating our program...\n\nThe cell below gathers up the Lab 3 code we ran in multiple cells.  \n\nIt reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.\n","user":"anonymous","dateUpdated":"2019-06-21T12:21:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 4</p>\n<h3>Simulating our program&hellip;</h3>\n<p>The cell below gathers up the Lab 3 code we ran in multiple cells. </p>\n<p>It reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017747_1175491187","id":"20190620-185404_1847570742","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:21:51+0000","dateFinished":"2019-06-21T12:21:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3704"},{"text":"%spark\n\n// cell 5\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");\n\n// Now let's turn it into a View so we can use SQL against it.\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n// IT ALL ACTS AS A SINGLE DATABASE!!!\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\") 5","user":"anonymous","dateUpdated":"2019-06-21T19:13:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017750_1174336940","id":"20190615-184855_149703656","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:55:02+0000","dateFinished":"2019-06-21T12:55:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3705"},{"text":"%spark\n\n// cell 6\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-21T12:56:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017757_1170104702","id":"20190620-201216_1498917214","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:56:23+0000","dateFinished":"2019-06-21T12:56:25+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3706"},{"text":"%cassandra\n\n// cell 7\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-21T12:56:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017758_1171258949","id":"20190620-202419_1221985005","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:56:41+0000","dateFinished":"2019-06-21T12:56:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3707"},{"text":"%cassandra\n\n// cell 8\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-21T12:56:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017759_1170874200","id":"20190620-202635_2085496124","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:56:50+0000","dateFinished":"2019-06-21T12:56:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3708","errorMessage":""},{"text":"%md\n\ncell 9\n\n### Examining the code\n\nLet's see how all of the above looks in a standalone Java program.\n\n```\npackage com.datastax.kellett;\n\nimport java.util.Scanner;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\npublic class Lab4 {\n    public static void main(String[] args) {\n\n        // Connect to Spark\n        System.out.println(\"About to build session...\");\n        SparkSession spark = SparkSession.builder()\n                .appName(\"Spark-SQL Lab 4\")           // any name you like... will display in Spark Management UI\n                .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate(); // gets existing spark session if present, or creates a new one\n\n        // We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n        // In the real world, this could be a flat file or perhaps a database table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(\"About to read transactions_buy.csv...\");\n        Dataset<Row> incoming_buy_transactions = spark.read().option(\"header\", true).csv(\"analytics_workshop.transactions_buy.csv\");\n\n        // Now let's turn it into a View so we can use SQL against it.\n        incoming_buy_transactions.createOrReplaceTempView(\"new_buys\");\n\n        // Next, let's whittle the table down so it only shows large transactions.\n        // Notice that we are now working in pure SQL.\n        Dataset<Row> newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\");\n\n        // The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(\"About to read transactions_sell.csv...\");\n        Dataset<Row> incoming_sell_transactions = spark.read().option(\"header\", true).csv(\"analytics_workshop.transactions_sell.csv\");\n        incoming_sell_transactions.createOrReplaceTempView(\"new_sells\");\n        Dataset<Row> newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\");\n\n        // Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table,\n        // and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n        // and then inserts the entire result into the (Cassandra) transactions_historical table.\n        // IT ALL ACTS AS A SINGLE DATABASE!!!\n        System.out.println(\"About to perform SQL INSERT operation...\");\n        Dataset<Row> ds = spark.sql(\"INSERT INTO analytics_workshop.transactions_historical \"\n                + \"SELECT \"\n                + \"c.account_number        AS account_number, \"\n                + \"t.transaction_id        AS transaction_id, \"\n                + \"c.city                  AS account_city, \"\n                + \"c.country               AS account_country, \"\n                + \"c.first_name            AS account_first_name, \"\n                + \"c.last_name             AS account_last_name, \"\n                + \"c.gender                AS account_gender, \"\n                + \"t.buy_or_sell           AS buy_or_sell, \"\n                + \"t.industry_sector       AS industry_sector, \"\n                + \"t.instrument_id         AS instrument_id, \"\n                + \"t.instrument_industry   AS instrument_industry, \"\n                + \"t.instrument_name       AS instrument_name, \"\n                + \"t.transaction_date      AS transaction_date, \"\n                + \"t.transaction_time      AS transaction_time, \"\n                + \"t.units                 AS units \"\n\n                + \"FROM new_buys t INNER JOIN analytics_workshop.customers c \"\n                + \"ON t.account_number = c.account_number \"\n                + \"WHERE t.units > 999 \"\n\n                + \"UNION \"\n\n                + \"SELECT \"\n                + \"c.account_number        AS account_number, \"\n                + \"t.transaction_id        AS transaction_id, \"\n                + \"c.city                  AS account_city, \"\n                + \"c.country               AS account_country, \"\n                + \"c.first_name            AS account_first_name, \"\n                + \"c.last_name             AS account_last_name, \"\n                + \"c.gender                AS account_gender, \"\n                + \"t.buy_or_sell           AS buy_or_sell, \"\n                + \"t.industry_sector       AS industry_sector, \"\n                + \"t.instrument_id         AS instrument_id, \"\n                + \"t.instrument_industry   AS instrument_industry, \"\n                + \"t.instrument_name       AS instrument_name, \"\n                + \"t.transaction_date      AS transaction_date, \"\n                + \"t.transaction_time      AS transaction_time, \"\n                + \"t.units                 AS units \"\n\n                + \"FROM new_sells t INNER JOIN analytics_workshop.customers c \"\n                + \"ON t.account_number = c.account_number \"\n                + \"WHERE t.units > 999\");\n\n        System.out.println(\"... SQL INSERT operation complete.\");\n\n        System.out.println(\"Waiting here to allow time for viewing Spark UI on port 7080.\");\n        System.out.println(\"Press enter when you want to terminate.\");\n        Scanner scanner = new Scanner(System.in);\n        scanner.nextLine();\n\n        spark.close();\n    }\n}\n\n```\n","user":"anonymous","dateUpdated":"2019-06-21T19:17:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 9</p>\n<h3>Examining the code</h3>\n<p>Let&rsquo;s see how all of the above looks in a standalone Java program.</p>\n<pre><code>package com.datastax.kellett;\n\nimport java.util.Scanner;\n\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\npublic class Lab4 {\n    public static void main(String[] args) {\n\n        // Connect to Spark\n        System.out.println(&quot;About to build session...&quot;);\n        SparkSession spark = SparkSession.builder()\n                .appName(&quot;Spark-SQL Lab 4&quot;)           // any name you like... will display in Spark Management UI\n                .enableHiveSupport()    // Enables connection to a Hive metastore, support for hive SerDes and Hive user-defined functions\n                .getOrCreate(); // gets existing spark session if present, or creates a new one\n\n        // We&#39;ll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n        // In the real world, this could be a flat file or perhaps a database table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(&quot;About to read transactions_buy.csv...&quot;);\n        Dataset&lt;Row&gt; incoming_buy_transactions = spark.read().option(&quot;header&quot;, true).csv(&quot;analytics_workshop.transactions_buy.csv&quot;);\n\n        // Now let&#39;s turn it into a View so we can use SQL against it.\n        incoming_buy_transactions.createOrReplaceTempView(&quot;new_buys&quot;);\n\n        // Next, let&#39;s whittle the table down so it only shows large transactions.\n        // Notice that we are now working in pure SQL.\n        Dataset&lt;Row&gt; newBuys = spark.sql(&quot;SELECT * FROM new_buys WHERE units &gt; 999&quot;);\n\n        // The new_buys table looks good.  Now let&#39;s do all the same things and make a new_sells table.\n        // NOTE that we placed this file on the DSEFS file system to make it accessible to all Spark nodes\n        System.out.println(&quot;About to read transactions_sell.csv...&quot;);\n        Dataset&lt;Row&gt; incoming_sell_transactions = spark.read().option(&quot;header&quot;, true).csv(&quot;analytics_workshop.transactions_sell.csv&quot;);\n        incoming_sell_transactions.createOrReplaceTempView(&quot;new_sells&quot;);\n        Dataset&lt;Row&gt; newSells = spark.sql(&quot;SELECT * FROM new_sells WHERE units &gt; 999&quot;);\n\n        // Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table,\n        // and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n        // and then inserts the entire result into the (Cassandra) transactions_historical table.\n        // IT ALL ACTS AS A SINGLE DATABASE!!!\n        System.out.println(&quot;About to perform SQL INSERT operation...&quot;);\n        Dataset&lt;Row&gt; ds = spark.sql(&quot;INSERT INTO analytics_workshop.transactions_historical &quot;\n                + &quot;SELECT &quot;\n                + &quot;c.account_number        AS account_number, &quot;\n                + &quot;t.transaction_id        AS transaction_id, &quot;\n                + &quot;c.city                  AS account_city, &quot;\n                + &quot;c.country               AS account_country, &quot;\n                + &quot;c.first_name            AS account_first_name, &quot;\n                + &quot;c.last_name             AS account_last_name, &quot;\n                + &quot;c.gender                AS account_gender, &quot;\n                + &quot;t.buy_or_sell           AS buy_or_sell, &quot;\n                + &quot;t.industry_sector       AS industry_sector, &quot;\n                + &quot;t.instrument_id         AS instrument_id, &quot;\n                + &quot;t.instrument_industry   AS instrument_industry, &quot;\n                + &quot;t.instrument_name       AS instrument_name, &quot;\n                + &quot;t.transaction_date      AS transaction_date, &quot;\n                + &quot;t.transaction_time      AS transaction_time, &quot;\n                + &quot;t.units                 AS units &quot;\n\n                + &quot;FROM new_buys t INNER JOIN analytics_workshop.customers c &quot;\n                + &quot;ON t.account_number = c.account_number &quot;\n                + &quot;WHERE t.units &gt; 999 &quot;\n\n                + &quot;UNION &quot;\n\n                + &quot;SELECT &quot;\n                + &quot;c.account_number        AS account_number, &quot;\n                + &quot;t.transaction_id        AS transaction_id, &quot;\n                + &quot;c.city                  AS account_city, &quot;\n                + &quot;c.country               AS account_country, &quot;\n                + &quot;c.first_name            AS account_first_name, &quot;\n                + &quot;c.last_name             AS account_last_name, &quot;\n                + &quot;c.gender                AS account_gender, &quot;\n                + &quot;t.buy_or_sell           AS buy_or_sell, &quot;\n                + &quot;t.industry_sector       AS industry_sector, &quot;\n                + &quot;t.instrument_id         AS instrument_id, &quot;\n                + &quot;t.instrument_industry   AS instrument_industry, &quot;\n                + &quot;t.instrument_name       AS instrument_name, &quot;\n                + &quot;t.transaction_date      AS transaction_date, &quot;\n                + &quot;t.transaction_time      AS transaction_time, &quot;\n                + &quot;t.units                 AS units &quot;\n\n                + &quot;FROM new_sells t INNER JOIN analytics_workshop.customers c &quot;\n                + &quot;ON t.account_number = c.account_number &quot;\n                + &quot;WHERE t.units &gt; 999&quot;);\n\n        System.out.println(&quot;... SQL INSERT operation complete.&quot;);\n\n        System.out.println(&quot;Waiting here to allow time for viewing Spark UI on port 7080.&quot;);\n        System.out.println(&quot;Press enter when you want to terminate.&quot;);\n        Scanner scanner = new Scanner(System.in);\n        scanner.nextLine();\n\n        spark.close();\n    }\n}\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017760_1156638491","id":"20190619-210148_86481232","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T19:17:37+0000","dateFinished":"2019-06-21T19:17:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3709"},{"text":"%md\n\ncell 10\n\n### Examining the build\n\nLet's examine the maven .pom file used to build the application.\n\n```\nstuff\n```","user":"anonymous","dateUpdated":"2019-06-21T19:19:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Examining the build</h3>\n<p>Let&rsquo;s examine the maven .pom file used to build the application.</p>\n<pre><code>stuff\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1561144663145_-1676095503","id":"20190621-191743_2003951309","dateCreated":"2019-06-21T19:17:43+0000","dateStarted":"2019-06-21T19:19:08+0000","dateFinished":"2019-06-21T19:19:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3710"},{"text":"%md\n\ncell 11\n\n### Running the code\n\nLet's submit our Spark job and see if it works.","user":"anonymous","dateUpdated":"2019-06-21T19:20:18+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 11</p>\n<h3>Running the code</h3>\n<p>Let&rsquo;s submit our Spark job and see if it works.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561121269454_-1813927104","id":"20190621-124749_1514972164","dateCreated":"2019-06-21T12:47:49+0000","dateStarted":"2019-06-21T19:20:18+0000","dateFinished":"2019-06-21T19:20:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3711"},{"text":"%cassandra\n\n// cell 12\n\n// Before we run the Spark job we'll truncate our destination table so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-21T19:21:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017768_1153560500","id":"20190620-220743_317732750","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T19:21:01+0000","dateFinished":"2019-06-21T19:21:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3712","errorMessage":""},{"text":"%cassandra\n\n// cell 13\n\n// Verify that the table has no rows\n\nselect count(*) from analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-21T19:23:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561139536930_-1725817291","id":"20190621-175216_909909353","dateCreated":"2019-06-21T17:52:16+0000","dateStarted":"2019-06-21T19:23:05+0000","dateFinished":"2019-06-21T19:23:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3713","errorMessage":""},{"text":"%cassandra\n\nselect * from analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-21T19:03:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561142707969_794821062","id":"20190621-184507_734246512","dateCreated":"2019-06-21T18:45:07+0000","dateStarted":"2019-06-21T19:03:01+0000","dateFinished":"2019-06-21T19:03:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3714","errorMessage":""},{"text":"%md\n\nNOTE: Spark Master is on port 7080\n\n```\ndse fs \"put file:/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv analytics_workshop.transactions_buy.csv\"\n\ndse fs \"put file:/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv analytics_workshop.transactions_sell.csv\"\n```","user":"anonymous","dateUpdated":"2019-06-21T19:39:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>NOTE: Spark Master is on port 7080</p>\n<pre><code>dse fs &quot;put file:/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv analytics_workshop.transactions_buy.csv&quot;\n\ndse fs &quot;put file:/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv analytics_workshop.transactions_sell.csv&quot;\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1561140138114_-504401521","id":"20190621-180218_1343810484","dateCreated":"2019-06-21T18:02:18+0000","dateStarted":"2019-06-21T19:39:47+0000","dateFinished":"2019-06-21T19:39:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3715"},{"text":"%md\n\ncell 21\n\n### Summary\n\nWe have seen how to create a complete Spark-SQL application in Java.\n","user":"anonymous","dateUpdated":"2019-06-21T12:53:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 21</p>\n<h3>Summary</h3>\n<p>We have seen how to create a complete Spark-SQL application in Java.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017769_1153175751","id":"20190620-221624_2092246902","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:53:08+0000","dateFinished":"2019-06-21T12:53:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3716"},{"text":"%md\ndse spark-submit --class com.datastax.kellett.Lab4 --total-executor-cores 2 --executor-memory 1g datastax-spark-sql-v2-jar-with-dependencies.jar","dateUpdated":"2019-06-21T19:13:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017790_1158946984","id":"20190620-223813_1558396189","dateCreated":"2019-06-21T12:10:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3717"}],"name":"Lab_4_-_Spark-SQL_Programming","id":"2EEKK79AK","angularObjects":{"2EEW72JQD:shared_process":[],"2ECWZ8P4Z:shared_process":[],"2EGK1SM5C:shared_process":[],"2EEJ782VX:shared_process":[],"2EE44WEQC:shared_process":[],"2EDPT5ZYR:shared_process":[],"2EFQTZ5CH:shared_process":[],"2EGER4KMH:shared_process":[],"2EFZQQQZ5:shared_process":[],"2EFU7BTNK:shared_process":[],"2EF6XDT2B:shared_process":[],"2EFZYT24A:shared_process":[],"2EGECCVPG:shared_process":[],"2EEMG3RGF:shared_process":[],"2EEH5QHCK:shared_process":[],"2EEKMFEUJ:shared_process":[],"2EDU9B892:shared_process":[],"2EDYDTYKF:shared_process":[],"2EF4RGVUJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}