{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 4 - Spark-SQL Programming**\n\nSo far, we have used Zeppelin notebooks to examine Spark-SQL capabilities.  We'll finish this workshop by creating a standalone Java program that uses Spark-SQL.\n\nOur standalone program will recreate some of the work we did in Lab 3.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement an ETL process that does the following:\n\n- **import transaction data** that originated from a business partner","user":"anonymous","dateUpdated":"2019-06-21T12:13:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 4 - Spark-SQL Programming</strong></h1>\n<p>So far, we have used Zeppelin notebooks to examine Spark-SQL capabilities. We&rsquo;ll finish this workshop by creating a standalone Java program that uses Spark-SQL.</p>\n<p>Our standalone program will recreate some of the work we did in Lab 3.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement an ETL process that does the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017724_1084305698","id":"20190615-193334_2123319074","dateCreated":"2019-06-21T12:10:17+0000","dateStarted":"2019-06-21T12:13:39+0000","dateFinished":"2019-06-21T12:13:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7699"},{"text":"%spark\n\n// cell 2\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 AND/OR LAB3 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","dateUpdated":"2019-06-21T12:53:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017746_1175875936","id":"20190615-195433_1606768454","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7700","user":"anonymous","dateFinished":"2019-06-21T12:54:01+0000","dateStarted":"2019-06-21T12:53:57+0000"},{"text":"%cassandra\n\n// cell 3\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","dateUpdated":"2019-06-21T12:54:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017749_1173182694","id":"20190620-185951_1579318164","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7702","focus":true,"user":"anonymous","dateFinished":"2019-06-21T12:54:16+0000","dateStarted":"2019-06-21T12:54:08+0000"},{"text":"%md\n\ncell 4\n\n### Simulating our program...\n\nThe cell below gathers up the Lab 3 code we ran in multiple cells.  \n\nIt reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.\n","dateUpdated":"2019-06-21T12:21:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 4</p>\n<h3>Simulating our program&hellip;</h3>\n<p>The cell below gathers up the Lab 3 code we ran in multiple cells. </p>\n<p>It reads in two flat files, joins them with a Cassandra table, and writes the result set to another Cassandra table.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017747_1175491187","id":"20190620-185404_1847570742","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7701","user":"anonymous","dateFinished":"2019-06-21T12:21:51+0000","dateStarted":"2019-06-21T12:21:51+0000"},{"text":"%spark\n\n// cell 5\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");\n\n// Now let's turn it into a View so we can use SQL against it.\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n// IT ALL ACTS AS A SINGLE DATABASE!!!\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\") ","dateUpdated":"2019-06-21T12:55:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false},"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017750_1174336940","id":"20190615-184855_149703656","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7703","user":"anonymous","dateFinished":"2019-06-21T12:55:17+0000","dateStarted":"2019-06-21T12:55:02+0000"},{"text":"%spark\n\n// cell 6\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","dateUpdated":"2019-06-21T12:56:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017757_1170104702","id":"20190620-201216_1498917214","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7710","user":"anonymous","dateFinished":"2019-06-21T12:56:25+0000","dateStarted":"2019-06-21T12:56:23+0000"},{"text":"%cassandra\n\n// cell 7\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-21T12:56:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017758_1171258949","id":"20190620-202419_1221985005","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7711","user":"anonymous","dateFinished":"2019-06-21T12:56:41+0000","dateStarted":"2019-06-21T12:56:41+0000"},{"text":"%cassandra\n\n// cell 8\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-21T12:56:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017759_1170874200","id":"20190620-202635_2085496124","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7712","user":"anonymous","dateFinished":"2019-06-21T12:56:50+0000","dateStarted":"2019-06-21T12:56:50+0000","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"count\n8978\n"}]}},{"text":"%md\n\ncell 9\n\n### Examining the code\n\nLet's see how all of the above looks in a standalone Java program.\n","dateUpdated":"2019-06-21T12:47:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 9</p>\n<h3>Examining the code</h3>\n<p>Let&rsquo;s see how all of the above looks in a standalone Java program.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017760_1156638491","id":"20190619-210148_86481232","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7713","user":"anonymous","dateFinished":"2019-06-21T12:47:26+0000","dateStarted":"2019-06-21T12:47:26+0000"},{"text":"%md\n\ncell 10\n\n### Running the code\n\nLet's submit our Spark job and see if it works.","user":"anonymous","dateUpdated":"2019-06-21T12:49:09+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561121269454_-1813927104","id":"20190621-124749_1514972164","dateCreated":"2019-06-21T12:47:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9058","dateFinished":"2019-06-21T12:49:06+0000","dateStarted":"2019-06-21T12:49:06+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 10</p>\n<h3>Running the code</h3>\n<p>Let&rsquo;s submit our Spark job and see if it works.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 11\n\n// First we'll truncate our destination table so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","dateUpdated":"2019-06-21T12:59:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017768_1153560500","id":"20190620-220743_317732750","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7716","user":"anonymous","dateFinished":"2019-06-21T12:59:39+0000","dateStarted":"2019-06-21T12:59:39+0000"},{"text":"%md\n\ncell 21\n\n### Summary\n\nWe have seen how to create a complete Spark-SQL application in Java.\n","dateUpdated":"2019-06-21T12:53:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 21</p>\n<h3>Summary</h3>\n<p>We have seen how to create a complete Spark-SQL application in Java.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561119017769_1153175751","id":"20190620-221624_2092246902","dateCreated":"2019-06-21T12:10:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7717","user":"anonymous","dateFinished":"2019-06-21T12:53:08+0000","dateStarted":"2019-06-21T12:53:08+0000"},{"text":"%md\n","dateUpdated":"2019-06-21T12:10:17+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561119017790_1158946984","id":"20190620-223813_1558396189","dateCreated":"2019-06-21T12:10:17+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7718"}],"name":"Lab_4_-_Spark-SQL_Programming","id":"2EEKK79AK","angularObjects":{"2EEW72JQD:shared_process":[],"2ECWZ8P4Z:shared_process":[],"2EGK1SM5C:shared_process":[],"2EEJ782VX:shared_process":[],"2EE44WEQC:shared_process":[],"2EDPT5ZYR:shared_process":[],"2EFQTZ5CH:shared_process":[],"2EGER4KMH:shared_process":[],"2EFZQQQZ5:shared_process":[],"2EFU7BTNK:shared_process":[],"2EF6XDT2B:shared_process":[],"2EFZYT24A:shared_process":[],"2EGECCVPG:shared_process":[],"2EEMG3RGF:shared_process":[],"2EEH5QHCK:shared_process":[],"2EEKMFEUJ:shared_process":[],"2EDU9B892:shared_process":[],"2EDYDTYKF:shared_process":[],"2EF4RGVUJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}