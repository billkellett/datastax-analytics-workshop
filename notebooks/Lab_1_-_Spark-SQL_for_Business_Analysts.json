{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 1 - Explore Spark-SQL**\n\nOur mission is to enable functionality needed by Business Analysts at RightVest.  In this lab we focus on empowering Business Analysts by providing them the ability to query DSE tables using **basic SQL operations**:\n\n### **Persona:** Niraj Gupta, a Business Intelligence Marketing Specialist at RightVest\n\nNiraj uses a data-driven approach to design marketing campaigns for RightVest.  He is very familiar with SQL, and wants to use it to find data he needs to tailor his marketing campaigns. \n\n### **User Story:** Find a set of customers to use in a Marketing campaign.\n\nLike many data analysts, Niraj takes an exploratory, iterative approach to finding the data he needs.  He likes to say that he \"tortures the data until it confesses.\"\n\nToday, Niraj wants to select sets of customers who are:\n\n- customers in **specific countries**\n- fairly **active traders**\n- interested in investing in companies that **avoid tobacco products**","user":"anonymous","dateUpdated":"2019-06-15T19:40:44+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560627214668_1884391536","id":"20190615-193334_2123319074","dateCreated":"2019-06-15T19:33:34+0000","dateStarted":"2019-06-15T19:40:44+0000","dateFinished":"2019-06-15T19:40:44+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:10409"},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine this schema.  As you can see, it is a very standard DSE schema, with Partition Keys, Clustering Keys, and data columns.","user":"anonymous","dateUpdated":"2019-06-15T19:47:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560627893224_-893313277","id":"20190615-194453_782238367","dateCreated":"2019-06-15T19:44:53+0000","dateStarted":"2019-06-15T19:46:58+0000","dateFinished":"2019-06-15T19:46:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10410"},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n\ndescribe keyspace analytics_workshop;","user":"anonymous","dateUpdated":"2019-06-15T19:47:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517327_-838725953","id":"20190529-161317_30181527","dateCreated":"2019-06-15T18:48:37+0000","dateStarted":"2019-06-15T19:47:58+0000","dateFinished":"2019-06-15T19:47:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10411"},{"text":"%md\n\ncell 4\n\n## What SQL functionality is supported?\n\nDSE uses Spark SQL capabilities, and Spark supports a large **subset** of the SQL specification.  \n\nDataStax partners with DataBricks to provide a fully-supported distribution of Spark.  DataBricks documentation provides a complete guide to supported SQL functionality.  \n\n**NOTE:** DSE does not include functionality marked as \"Delta.\"\n\n[To access documentation, right-click here and select \"Open in new tab.\"](https://docs.databricks.com/spark/latest/spark-sql/index.html#sql-language-manual) \n\n**Discussion questions:**\n\n- What are some key areas of functionality that are _not_ supported?\n- Think about our \"just enough internals\" section in the introductory presentation.  Does this explain why some functionality may not be supported?\n- Which of these areas might be critical for your needs?  Can you think of workarounds?\n\nLet's get our feet wet by trying out some common query types.  Then we'll try to build a query that Niraj might actually use in his marketing campaigns.\n","user":"anonymous","dateUpdated":"2019-06-15T19:50:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560628110789_-1853724797","id":"20190615-194830_1099590306","dateCreated":"2019-06-15T19:48:30+0000","dateStarted":"2019-06-15T19:50:05+0000","dateFinished":"2019-06-15T19:50:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10412"},{"text":"%spark\n\n// cell 5\n\n// RUN THIS ONLY ONCE PER SESSION\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW countries USING org.apache.spark.sql.cassandra OPTIONS ( table 'countries', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-16T14:51:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560628473646_207844653","id":"20190615-195433_1606768454","dateCreated":"2019-06-15T19:54:33+0000","dateStarted":"2019-06-16T14:06:21+0000","dateFinished":"2019-06-16T14:06:22+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10413"},{"text":"%spark\n\n// cell 6\n\n// Now we can try a very basic SQL query\n\nvar ds = spark.sql(\"SELECT * FROM customers LIMIT 10\");\nds.show","user":"anonymous","dateUpdated":"2019-06-15T20:04:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624535154_806106243","id":"20190615-184855_149703656","dateCreated":"2019-06-15T18:48:55+0000","dateStarted":"2019-06-15T20:04:08+0000","dateFinished":"2019-06-15T20:04:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10414"},{"text":"%md\n\ncell 7\n\nCheck out the **response times above** on the Spark SQL version, and compare them to the same query in CQL. Note that the CQL version is much faster.  That's because there is some setup overhead in Spark.  The setup time becomes trivial when you are running large queries, but may be noticeable when you are running small queries.  Also keep in mind that your session's very first execution may take longer than subsequent executions.\n\nLet's try some more ambitious queries.\n","user":"anonymous","dateUpdated":"2019-06-15T20:11:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560629401011_-68467475","id":"20190615-201001_370488975","dateCreated":"2019-06-15T20:10:01+0000","dateStarted":"2019-06-15T20:10:59+0000","dateFinished":"2019-06-15T20:10:59+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10415"},{"text":"%spark\n\n// cell 8\n\n// Now let's see if we can get fancy with a single-table query\n\nvar ds = spark.sql(\"select  account_number, \"\n                         + \"UPPER(first_name), \"\n                         + \"UPPER(last_name), \"\n                         + \"country, \"\n                         + \"avoids_tobacco_min \"\n                     + \"from customers \"\n                     + \"where country IN ('Sweden', 'Norway', 'Denmark') \"\n                         + \"and avoids_tobacco_min > 3 \"\n                     + \"order by country asc, account_number asc\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T13:01:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560629492075_-1355746994","id":"20190615-201132_1184275073","dateCreated":"2019-06-15T20:11:32+0000","dateStarted":"2019-06-16T13:01:03+0000","dateFinished":"2019-06-16T13:01:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10416"},{"text":"%spark\n\n// cell 9\n\n// Now let's try a sub-select in a WHERE clause\n\nvar ds = spark.sql(\"select  account_number, \" \n                         + \"UPPER(first_name), \"\n                         + \"UPPER(last_name), \"\n                         + \"country, \"\n                         + \"avoids_tobacco_min \"\n                    + \"from customers \"\n                    + \"where country IN (select * from countries where country LIKE 'S%') \"\n                    + \"and avoids_tobacco_min > 3\") \nds.show","user":"anonymous","dateUpdated":"2019-06-16T13:08:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560690068842_-1598093723","id":"20190616-130108_1826314840","dateCreated":"2019-06-16T13:01:08+0000","dateStarted":"2019-06-16T13:08:25+0000","dateFinished":"2019-06-16T13:08:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10417"},{"text":"%spark\n\n// cell 10\n\n// Let's try an aggregation\n\nvar ds = spark.sql(\"select  country, \" \n                         + \"cast(avg(avoids_tobacco_min) as decimal(3,2)) as average_tobacco_concern, \"\n                         + \"count(account_number) as total_customers \"\n                     + \"from customers \"\n                     + \"group by country \"\n                     + \"order by total_customers desc, country asc\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T13:53:54+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560692949085_865959706","id":"20190616-134909_1475086077","dateCreated":"2019-06-16T13:49:09+0000","dateStarted":"2019-06-16T13:53:54+0000","dateFinished":"2019-06-16T13:53:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10418"},{"text":"%spark\n\n// cell 11\n\n// Do we dare try a Join in DSE?  Yes we do!!!\n\nvar ds = spark.sql(\"select \"  \n                    + \"c.account_number as acct_num, \"\n                    + \"transaction_id, \"\n                    + \"account_country, \"\n                    + \"account_first_name, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"buy_or_sell, \"\n                    + \"units \"\n\n                + \"from \"\n                    + \"transactions t \"\n                    + \"INNER JOIN \"\n                    + \"customers c \"\n        \n                + \"on \"\n                    + \"c.account_number \"\n                    + \"= \"\n                    + \"t.account_number \"\n        \n                + \"order by \"\n                    + \"acct_num asc, \"\n                    + \"transaction_date desc, \"\n                    + \"transaction_time desc\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:06:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560693274274_1856760932","id":"20190616-135434_246632690","dateCreated":"2019-06-16T13:54:34+0000","dateStarted":"2019-06-16T14:06:57+0000","dateFinished":"2019-06-16T14:07:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10419"},{"text":"%spark\n\n// cell 12\n\n// Now let's try a Union\n\nvar ds = spark.sql(\"select  \"\n                    + \"account_number, \"\n                    + \"buy_or_sell, \"\n                    + \"transaction_id, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"units \"\n    \n                + \"from \"\n                    + \"transactions_buy \"\n\n                + \"UNION \"\n\n                + \"select  \"\n                    + \"account_number, \"\n                    + \"buy_or_sell, \"\n                    + \"transaction_id, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"units \"\n    \n                + \"from \"\n                    + \"transactions_sell\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:13:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694040865_1785485507","id":"20190616-140720_1763039505","dateCreated":"2019-06-16T14:07:20+0000","dateStarted":"2019-06-16T14:13:33+0000","dateFinished":"2019-06-16T14:13:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10420"},{"text":"%md\n\ncell 13\n\nWe have examined a pretty wide range of read operations.  \n\nNow let's take a look at some **write operations**.","user":"anonymous","dateUpdated":"2019-06-16T14:15:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694438348_785879384","id":"20190616-141358_1306496002","dateCreated":"2019-06-16T14:13:58+0000","dateStarted":"2019-06-16T14:15:23+0000","dateFinished":"2019-06-16T14:15:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10421"},{"text":"%spark\n\n// cell 14\n\n// get the original count of the countries table\n// should be 124, but may be 125 if you have run this notebook previously\n\nvar ds = spark.sql(\"select count(*) from countries\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T14:17:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694536075_-699731353","id":"20190616-141536_497145118","dateCreated":"2019-06-16T14:15:36+0000","dateStarted":"2019-06-16T14:17:04+0000","dateFinished":"2019-06-16T14:17:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10422"},{"text":"%spark\n\n// cell 15\n\n// add a country row\n\nvar ds = spark.sql(\"insert into countries values ('test_country')\")","user":"anonymous","dateUpdated":"2019-06-16T14:20:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694647819_-539490276","id":"20190616-141727_784418913","dateCreated":"2019-06-16T14:17:27+0000","dateStarted":"2019-06-16T14:20:28+0000","dateFinished":"2019-06-16T14:20:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10423"},{"text":"%spark\n\n// cell 16\n\n// get the new row count of the countries table\n\nvar ds = spark.sql(\"select count(*) from countries\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:20:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694749604_1289560023","id":"20190616-141909_1865478010","dateCreated":"2019-06-16T14:19:09+0000","dateStarted":"2019-06-16T14:20:35+0000","dateFinished":"2019-06-16T14:20:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10424"},{"text":"%md\n\ncell 17\n\n**Question:** What happens to the row count if you run the above INSERT multiple times? Why? (Hint… think “upsert”).\n","user":"anonymous","dateUpdated":"2019-06-16T14:22:19+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694863800_958427493","id":"20190616-142103_1866710569","dateCreated":"2019-06-16T14:21:03+0000","dateStarted":"2019-06-16T14:22:19+0000","dateFinished":"2019-06-16T14:22:19+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10425"},{"text":"%spark\n\n// cell 18\n\n// read our new row, just to verify\n\nvar ds = spark.sql(\"select * from countries where country = 'test_country'\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T14:24:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694988453_-610008238","id":"20190616-142308_693602846","dateCreated":"2019-06-16T14:23:08+0000","dateStarted":"2019-06-16T14:24:04+0000","dateFinished":"2019-06-16T14:24:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10426"},{"text":"%md\n\ncell 19\n\nWe already know that DELETEs and UPDATEs are not supported in Spark SQL, but let's see how we might select a subset, massage the data, and persist it to a new table for future analysis purposes.","user":"anonymous","dateUpdated":"2019-06-16T14:36:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695359881_1200166938","id":"20190616-142919_1147304688","dateCreated":"2019-06-16T14:29:19+0000","dateStarted":"2019-06-16T14:36:12+0000","dateFinished":"2019-06-16T14:36:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10427"},{"text":"%spark\n\n// cell 20\n\n// make sure our new table does not already exist\n\nvar ds = spark.sql(\"drop table if exists countries_2\")\n","user":"anonymous","dateUpdated":"2019-06-16T14:37:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695798985_494382569","id":"20190616-143638_1149140415","dateCreated":"2019-06-16T14:36:38+0000","dateStarted":"2019-06-16T14:37:41+0000","dateFinished":"2019-06-16T14:37:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10428"},{"text":"%spark\n\n// cell 21\n\n// Here we create a new table that uses all rows from the country table EXCEPT the new test row we inserted\n// Notice that we do not have to explicitly define the columns in the new table.  This is a very convenient way to persist our data for analytics.\n\nvar ds = spark.sql(\"create table countries_2 as select * from countries where country <> 'test_country'\")","user":"anonymous","dateUpdated":"2019-06-16T14:40:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695927275_-778215195","id":"20190616-143847_815108206","dateCreated":"2019-06-16T14:38:47+0000","dateStarted":"2019-06-16T14:40:07+0000","dateFinished":"2019-06-16T14:40:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10429"},{"text":"%spark\n\n// cell 22\n\n// Notice that only 124 rows are returned, because our test row was excluded from the new table\n\nvar ds = spark.sql(\"select * from countries_2\")\nds.show\n\nvar ds = spark.sql(\"select count(*) from countries_2\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:44:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560696049395_-204353726","id":"20190616-144049_441394643","dateCreated":"2019-06-16T14:40:49+0000","dateStarted":"2019-06-16T14:44:49+0000","dateFinished":"2019-06-16T14:44:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10430"},{"text":"%md\n\ncell 23\n\n## Helping Niraj with his Marketing campaign...\n\nSo far we've been experimenting with our SQL capabilities.  Now, let's see how Niraj uses them to accomplish his task.\n\nToday, Niraj wants to select sets of customers who are:\n\n- customers in **specific countries**\n- fairly **active traders**\n- interested in investing in companies that **avoid tobacco products**\n\nHe wants **two tables:** \n\n- **detail-level** information (one row per customer transaction)\n- **aggregate-level** information (one row per customer, in descending order by number of transactions executed)\n\nHe can then use these two tables to do interesting data manipulation in his external tool of choice.\n\nLet's begin with the detail-level record.\n","user":"anonymous","dateUpdated":"2019-06-16T18:18:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560696469195_442193515","id":"20190616-144749_872651136","dateCreated":"2019-06-16T14:47:49+0000","dateStarted":"2019-06-16T18:18:58+0000","dateFinished":"2019-06-16T18:18:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10431"},{"text":"%spark\n\n// cell 24\n\n// detail-level information\n\nvar ds = spark.sql(\"select \"\n                    + \"c.account_number as acct_num, \"\n                    + \"transaction_id, \"\n                    + \"account_country, \"\n                    + \"account_city, \"\n                    + \"account_first_name, \"\n                    + \"account_last_name, \"\n                    + \"avoids_tobacco_min, \"\n                    + \"email, \"\n                    + \"phone, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"buy_or_sell, \"\n                    + \"units \"\n\n                + \"from \"\n                    + \"transactions t \"\n                    + \"INNER JOIN \"\n                    + \"customers c \"\n        \n                + \"on \"\n                    + \"c.account_number \"\n                    + \"= \"\n                    + \"t.account_number \"\n        \n                + \"where \"\n                    + \"buy_or_sell = 'BUY' \"\n                    + \"AND \"\n                    + \"account_country IN ('Norway', 'Denmark', 'Sweden') \"\n                    + \"AND \"\n                    + \"avoids_tobacco_min > 2 \"\n        \n                + \"order by \"\n                    + \"acct_num asc, \"\n                    + \"transaction_date desc, \"\n                    + \"transaction_time desc\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T18:27:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709169276_111907849","id":"20190616-181929_1380794411","dateCreated":"2019-06-16T18:19:29+0000","dateStarted":"2019-06-16T18:27:51+0000","dateFinished":"2019-06-16T18:27:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10432"},{"text":"%md\n\ncell 25\n\nThe query above gives a current result every time Niraj runs it.\n\nBut now suppose Niraj wants to **persist a snapshot** of his query...\n","user":"anonymous","dateUpdated":"2019-06-16T18:29:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709716563_431600247","id":"20190616-182836_684269486","dateCreated":"2019-06-16T18:28:36+0000","dateStarted":"2019-06-16T18:29:05+0000","dateFinished":"2019-06-16T18:29:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10433"},{"text":"%spark\n\n// cell 26\n\n// drop table just in case we have run this notebook before\n\nvar ds = spark.sql(\"drop table if exists campaign_1_detail\")","user":"anonymous","dateUpdated":"2019-06-16T18:31:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709790472_1133305620","id":"20190616-182950_627980856","dateCreated":"2019-06-16T18:29:50+0000","dateStarted":"2019-06-16T18:31:07+0000","dateFinished":"2019-06-16T18:31:07+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10434"},{"text":"%spark\n\n// These imports not needed for Spark-SQL, but I do need them in order to do rdd-level C* table work\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.cql.CassandraConnector\n\n// These imports not needed, but including them because example code uses them\n// (WriteRead.scala in SparkBuildExamples in my Intellij)\n//import org.apache.spark.sql.{SaveMode, SparkSession}\n//import org.apache.spark.sql.cassandra._","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517326_-838341204","id":"20190615-181459_1816041684","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10435"},{"text":"%spark\n\n// create a literal RDD just to prove that spark access is working\n\nval list = List( (\"Frozen\", 2013), (\"Toy Story\", 1995), (\"WALL-E\", 2008) )\nval rdd = sc.parallelize(list)\nrdd.take(3)\n","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517327_-838725953","id":"20190529-160118_1285216078","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10436"},{"text":"%cassandra\n\n// prove that you can read the countries table with cql\n\nselect * from analytics_workshop.countries;","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517327_-838725953","id":"20190530-142647_1892250973","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10437"},{"text":"%spark\n\n// prove that you can get a Spark SQLContext (I think this is deprecated)\n\nsqlContext\n","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517328_-828337733","id":"20190530-144334_508411187","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10438"},{"text":"%spark\n\n// prove that you can get a Spark Session (I think I'm supposed to use this instead of SQLContext)\n\nspark","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517328_-828337733","id":"20190530-145225_2110572036","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10439"},{"text":"%spark\n\n// prove that you can do a low-level read from C*\n\nval tableRDD = sc.cassandraTable(\"analytics_workshop\", \"countries\")\ntableRDD.take(10)","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":87,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517328_-828337733","id":"20190530-145409_300100572","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10440"},{"text":"%spark\n\n// prove that you can use the Cassandra Connector\n\nval qry = sc.cassandraTable(\"analytics_workshop\", \"countries\").select(\"country\").where(\"country = 'Madagascar'\")\nval resultRDD = qry.collect\nresultRDD.foreach(row => println(row.getString(\"country\")))","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517329_-828722482","id":"20190530-152318_1800161658","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10441"},{"text":"%spark\n\n// DO THIS ONLY ONCE PER SESSION!!!\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use\n// I *think* you do not need this if you do a dse spark-submit\n\nvar createDDL = \"CREATE TEMPORARY VIEW countries USING org.apache.spark.sql.cassandra OPTIONS ( table 'countries', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\";\n\nspark.sql(createDDL);\n\t\t","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517329_-828722482","id":"20190530-160147_749487410","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10442"},{"text":"%spark\n\n// prove that you can query with Spark SQL\n\nvar ds = spark.sql(\"SELECT * FROM countries WHERE country = 'Madagascar'\");\nds.show\n","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517330_-827568235","id":"20190530-192655_1285746303","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10443"},{"text":"%spark\n","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517330_-827568235","id":"20190530-194003_392853198","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:10444"}],"name":"Lab_1_-_Spark-SQL_for_Business_Analysts","id":"2EERXCB6B","angularObjects":{"2EG82R1CT:shared_process":[],"2EGDJGTTF:shared_process":[],"2EDDSRFGQ:shared_process":[],"2EDQSPH14:shared_process":[],"2EGANEBGU:shared_process":[],"2EFGW11TW:shared_process":[],"2ECK9UCRJ:shared_process":[],"2ED1GX1AD:shared_process":[],"2EEYRTZAY:shared_process":[],"2EEE1MG63:shared_process":[],"2EFY218XD:shared_process":[],"2EE2ZX7EQ:shared_process":[],"2EDKUCTN1:shared_process":[],"2EEG6D3PT:shared_process":[],"2EEV6VR86:shared_process":[],"2EF1XKYD2:shared_process":[],"2EEDVMK8C:shared_process":[],"2EDVP4F4X:shared_process":[],"2EFSSKAXV:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}