{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 1 - Explore Spark-SQL**\n\nOur mission is to enable functionality needed by Business Analysts at RightVest.  In this lab we focus on empowering Business Analysts by providing them the ability to query DSE tables using **basic SQL operations**:\n\n### **Persona:** Niraj Gupta, a Business Intelligence Marketing Specialist at RightVest\n\nNiraj uses a data-driven approach to design marketing campaigns for RightVest.  He is very familiar with SQL, and wants to use it to find data he needs to tailor his marketing campaigns. \n\n### **User Story:** Find a set of customers to use in a Marketing campaign.\n\nLike many data analysts, Niraj takes an exploratory, iterative approach to finding the data he needs.  He likes to say that he \"tortures the data until it confesses.\"\n\nToday, Niraj wants to select sets of customers who are:\n\n- customers in **specific countries**\n- fairly **active traders**\n- interested in investing in companies that **avoid tobacco products**","user":"anonymous","dateUpdated":"2019-06-17T16:09:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560627214668_1884391536","id":"20190615-193334_2123319074","dateCreated":"2019-06-15T19:33:34+0000","dateStarted":"2019-06-17T16:09:49+0000","dateFinished":"2019-06-17T16:09:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:309","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 1 - Explore Spark-SQL</strong></h1>\n<p>Our mission is to enable functionality needed by Business Analysts at RightVest. In this lab we focus on empowering Business Analysts by providing them the ability to query DSE tables using <strong>basic SQL operations</strong>:</p>\n<h3><strong>Persona:</strong> Niraj Gupta, a Business Intelligence Marketing Specialist at RightVest</h3>\n<p>Niraj uses a data-driven approach to design marketing campaigns for RightVest. He is very familiar with SQL, and wants to use it to find data he needs to tailor his marketing campaigns. </p>\n<h3><strong>User Story:</strong> Find a set of customers to use in a Marketing campaign.</h3>\n<p>Like many data analysts, Niraj takes an exploratory, iterative approach to finding the data he needs. He likes to say that he &ldquo;tortures the data until it confesses.&rdquo;</p>\n<p>Today, Niraj wants to select sets of customers who are:</p>\n<ul>\n  <li>customers in <strong>specific countries</strong></li>\n  <li>fairly <strong>active traders</strong></li>\n  <li>interested in investing in companies that <strong>avoid tobacco products</strong></li>\n</ul>\n</div>"}]}},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine this schema.  As you can see, it is a very standard DSE schema, with Partition Keys, Clustering Keys, and data columns.","user":"anonymous","dateUpdated":"2019-06-17T16:09:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560627893224_-893313277","id":"20190615-194453_782238367","dateCreated":"2019-06-15T19:44:53+0000","dateStarted":"2019-06-17T16:09:53+0000","dateFinished":"2019-06-17T16:09:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:310","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 2</p>\n<h2>Examining our data&hellip;</h2>\n<p>Execute the cell below to examine the Cassandra Keyspace we will be using in this workshop..</p>\n<p>Take a few minutes to examine this schema. As you can see, it is a very standard DSE schema, with Partition Keys, Clustering Keys, and data columns.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n\ndescribe keyspace analytics_workshop;","user":"anonymous","dateUpdated":"2019-06-15T19:47:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517327_-838725953","id":"20190529-161317_30181527","dateCreated":"2019-06-15T18:48:37+0000","dateStarted":"2019-06-15T19:47:58+0000","dateFinished":"2019-06-15T19:47:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"%md\n\ncell 4\n\n## What SQL functionality is supported?\n\nDSE uses Spark SQL capabilities, and Spark supports a large **subset** of the SQL specification.  \n\nDataStax partners with DataBricks to provide a fully-supported distribution of Spark.  DataBricks documentation provides a complete guide to supported SQL functionality.  \n\n**NOTE:** DSE does not include functionality marked as \"Delta.\"\n\n[To access documentation, right-click here and select \"Open in new tab.\"](https://docs.databricks.com/spark/latest/spark-sql/index.html#sql-language-manual) \n\n**Discussion questions:**\n\n- What are some key areas of functionality that are _not_ supported?\n- Think about our \"just enough internals\" section in the introductory presentation.  Does this explain why some functionality may not be supported?\n- Which of these areas might be critical for your needs?  Can you think of workarounds?\n\nLet's get our feet wet by trying out some common query types.  Then we'll try to build a query that Niraj might actually use in his marketing campaigns.\n","user":"anonymous","dateUpdated":"2019-06-17T16:09:59+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560628110789_-1853724797","id":"20190615-194830_1099590306","dateCreated":"2019-06-15T19:48:30+0000","dateStarted":"2019-06-17T16:09:59+0000","dateFinished":"2019-06-17T16:09:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 4</p>\n<h2>What SQL functionality is supported?</h2>\n<p>DSE uses Spark SQL capabilities, and Spark supports a large <strong>subset</strong> of the SQL specification. </p>\n<p>DataStax partners with DataBricks to provide a fully-supported distribution of Spark. DataBricks documentation provides a complete guide to supported SQL functionality. </p>\n<p><strong>NOTE:</strong> DSE does not include functionality marked as &ldquo;Delta.&rdquo;</p>\n<p><a href=\"https://docs.databricks.com/spark/latest/spark-sql/index.html#sql-language-manual\">To access documentation, right-click here and select &ldquo;Open in new tab.&rdquo;</a> </p>\n<p><strong>Discussion questions:</strong></p>\n<ul>\n  <li>What are some key areas of functionality that are <em>not</em> supported?</li>\n  <li>Think about our &ldquo;just enough internals&rdquo; section in the introductory presentation. Does this explain why some functionality may not be supported?</li>\n  <li>Which of these areas might be critical for your needs? Can you think of workarounds?</li>\n</ul>\n<p>Let&rsquo;s get our feet wet by trying out some common query types. Then we&rsquo;ll try to build a query that Niraj might actually use in his marketing campaigns.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 5\n\n// RUN THIS ONLY ONCE PER SESSION\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW countries USING org.apache.spark.sql.cassandra OPTIONS ( table 'countries', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-16T14:51:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560628473646_207844653","id":"20190615-195433_1606768454","dateCreated":"2019-06-15T19:54:33+0000","dateStarted":"2019-06-16T14:06:21+0000","dateFinished":"2019-06-16T14:06:22+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"text":"%spark\n\n// cell 6\n\n// Now we can try a very basic SQL query\n\nvar ds = spark.sql(\"SELECT * FROM customers LIMIT 10\");\nds.show","user":"anonymous","dateUpdated":"2019-06-15T20:04:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624535154_806106243","id":"20190615-184855_149703656","dateCreated":"2019-06-15T18:48:55+0000","dateStarted":"2019-06-15T20:04:08+0000","dateFinished":"2019-06-15T20:04:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"text":"%md\n\ncell 7\n\nCheck out the **response times above** on the Spark SQL version, and compare them to the same query in CQL. Note that the CQL version is much faster.  That's because there is some setup overhead in Spark.  The setup time becomes trivial when you are running large queries, but may be noticeable when you are running small queries.  Also keep in mind that your session's very first execution may take longer than subsequent executions.\n\nLet's try some more ambitious queries.\n","user":"anonymous","dateUpdated":"2019-06-17T17:29:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560629401011_-68467475","id":"20190615-201001_370488975","dateCreated":"2019-06-15T20:10:01+0000","dateStarted":"2019-06-17T17:29:50+0000","dateFinished":"2019-06-17T17:29:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:315","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 7</p>\n<p>Check out the <strong>response times above</strong> on the Spark SQL version, and compare them to the same query in CQL. Note that the CQL version is much faster. That&rsquo;s because there is some setup overhead in Spark. The setup time becomes trivial when you are running large queries, but may be noticeable when you are running small queries. Also keep in mind that your session&rsquo;s very first execution may take longer than subsequent executions.</p>\n<p>Let&rsquo;s try some more ambitious queries.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 8\n\n// Now let's see if we can get fancy with a single-table query\n\nvar ds = spark.sql(\"select  account_number, \"\n                         + \"UPPER(first_name), \"\n                         + \"UPPER(last_name), \"\n                         + \"country, \"\n                         + \"avoids_tobacco_min \"\n                     + \"from customers \"\n                     + \"where country IN ('Sweden', 'Norway', 'Denmark') \"\n                         + \"and avoids_tobacco_min > 3 \"\n                     + \"order by country asc, account_number asc\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T13:01:03+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560629492075_-1355746994","id":"20190615-201132_1184275073","dateCreated":"2019-06-15T20:11:32+0000","dateStarted":"2019-06-16T13:01:03+0000","dateFinished":"2019-06-16T13:01:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:316"},{"text":"%spark\n\n// cell 9\n\n// Now let's try a sub-select in a WHERE clause\n\nvar ds = spark.sql(\"select  account_number, \" \n                         + \"UPPER(first_name), \"\n                         + \"UPPER(last_name), \"\n                         + \"country, \"\n                         + \"avoids_tobacco_min \"\n                    + \"from customers \"\n                    + \"where country IN (select * from countries where country LIKE 'S%') \"\n                    + \"and avoids_tobacco_min > 3\") \nds.show","user":"anonymous","dateUpdated":"2019-06-16T13:08:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560690068842_-1598093723","id":"20190616-130108_1826314840","dateCreated":"2019-06-16T13:01:08+0000","dateStarted":"2019-06-16T13:08:25+0000","dateFinished":"2019-06-16T13:08:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:317"},{"text":"%spark\n\n// cell 10\n\n// Let's try an aggregation\n\nvar ds = spark.sql(\"select  country, \" \n                         + \"cast(avg(avoids_tobacco_min) as decimal(3,2)) as average_tobacco_concern, \"\n                         + \"count(account_number) as total_customers \"\n                     + \"from customers \"\n                     + \"group by country \"\n                     + \"order by total_customers desc, country asc\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T13:53:54+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560692949085_865959706","id":"20190616-134909_1475086077","dateCreated":"2019-06-16T13:49:09+0000","dateStarted":"2019-06-16T13:53:54+0000","dateFinished":"2019-06-16T13:53:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"text":"%spark\n\n// cell 11\n\n// Do we dare try a Join in DSE?  Yes we do!!!\n\nvar ds = spark.sql(\"select \"  \n                    + \"c.account_number as acct_num, \"\n                    + \"transaction_id, \"\n                    + \"account_country, \"\n                    + \"account_first_name, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"buy_or_sell, \"\n                    + \"units \"\n\n                + \"from \"\n                    + \"transactions t \"\n                    + \"INNER JOIN \"\n                    + \"customers c \"\n        \n                + \"on \"\n                    + \"c.account_number \"\n                    + \"= \"\n                    + \"t.account_number \"\n        \n                + \"order by \"\n                    + \"acct_num asc, \"\n                    + \"transaction_date desc, \"\n                    + \"transaction_time desc\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:06:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560693274274_1856760932","id":"20190616-135434_246632690","dateCreated":"2019-06-16T13:54:34+0000","dateStarted":"2019-06-16T14:06:57+0000","dateFinished":"2019-06-16T14:07:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"%spark\n\n// cell 12\n\n// Now let's try a Union\n\nvar ds = spark.sql(\"select  \"\n                    + \"account_number, \"\n                    + \"buy_or_sell, \"\n                    + \"transaction_id, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"units \"\n    \n                + \"from \"\n                    + \"transactions_buy \"\n\n                + \"UNION \"\n\n                + \"select  \"\n                    + \"account_number, \"\n                    + \"buy_or_sell, \"\n                    + \"transaction_id, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"units \"\n    \n                + \"from \"\n                    + \"transactions_sell\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:13:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694040865_1785485507","id":"20190616-140720_1763039505","dateCreated":"2019-06-16T14:07:20+0000","dateStarted":"2019-06-16T14:13:33+0000","dateFinished":"2019-06-16T14:13:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"text":"%md\n\ncell 13\n\nWe have examined a pretty wide range of read operations.  \n\nNow let's take a look at some **write operations**.","user":"anonymous","dateUpdated":"2019-06-17T16:10:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694438348_785879384","id":"20190616-141358_1306496002","dateCreated":"2019-06-16T14:13:58+0000","dateStarted":"2019-06-17T16:10:35+0000","dateFinished":"2019-06-17T16:10:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:321","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 13</p>\n<p>We have examined a pretty wide range of read operations. </p>\n<p>Now let&rsquo;s take a look at some <strong>write operations</strong>.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 14\n\n// get the original count of the countries table\n// should be 124, but may be 125 if you have run this notebook previously\n\nvar ds = spark.sql(\"select count(*) from countries\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T14:17:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694536075_-699731353","id":"20190616-141536_497145118","dateCreated":"2019-06-16T14:15:36+0000","dateStarted":"2019-06-16T14:17:04+0000","dateFinished":"2019-06-16T14:17:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:322"},{"text":"%spark\n\n// cell 15\n\n// add a country row\n\nvar ds = spark.sql(\"insert into countries values ('test_country')\")","user":"anonymous","dateUpdated":"2019-06-16T14:20:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694647819_-539490276","id":"20190616-141727_784418913","dateCreated":"2019-06-16T14:17:27+0000","dateStarted":"2019-06-16T14:20:28+0000","dateFinished":"2019-06-16T14:20:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"text":"%spark\n\n// cell 16\n\n// get the new row count of the countries table\n\nvar ds = spark.sql(\"select count(*) from countries\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:20:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694749604_1289560023","id":"20190616-141909_1865478010","dateCreated":"2019-06-16T14:19:09+0000","dateStarted":"2019-06-16T14:20:35+0000","dateFinished":"2019-06-16T14:20:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%md\n\ncell 17\n\n**Question:** What happens to the row count if you run the above INSERT multiple times? Why? (Hint… think “upsert”).\n","user":"anonymous","dateUpdated":"2019-06-17T16:10:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694863800_958427493","id":"20190616-142103_1866710569","dateCreated":"2019-06-16T14:21:03+0000","dateStarted":"2019-06-17T16:10:30+0000","dateFinished":"2019-06-17T16:10:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:325","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 17</p>\n<p><strong>Question:</strong> What happens to the row count if you run the above INSERT multiple times? Why? (Hint… think “upsert”).</p>\n</div>"}]}},{"text":"%spark\n\n// cell 18\n\n// read our new row, just to verify\n\nvar ds = spark.sql(\"select * from countries where country = 'test_country'\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T14:24:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560694988453_-610008238","id":"20190616-142308_693602846","dateCreated":"2019-06-16T14:23:08+0000","dateStarted":"2019-06-16T14:24:04+0000","dateFinished":"2019-06-16T14:24:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"%md\n\ncell 19\n\nWe already know that DELETEs and UPDATEs are not supported in Spark SQL, but let's see how we might select a subset, massage the data, and persist it to a new table for future analysis purposes.","user":"anonymous","dateUpdated":"2019-06-17T16:10:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","tableHide":false,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695359881_1200166938","id":"20190616-142919_1147304688","dateCreated":"2019-06-16T14:29:19+0000","dateStarted":"2019-06-17T16:10:25+0000","dateFinished":"2019-06-17T16:10:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:327","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 19</p>\n<p>We already know that DELETEs and UPDATEs are not supported in Spark SQL, but let&rsquo;s see how we might select a subset, massage the data, and persist it to a new table for future analysis purposes.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 20\n\n// make sure our new table does not already exist\n\nvar ds = spark.sql(\"drop table if exists countries_2\")\n","user":"anonymous","dateUpdated":"2019-06-16T14:37:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695798985_494382569","id":"20190616-143638_1149140415","dateCreated":"2019-06-16T14:36:38+0000","dateStarted":"2019-06-16T14:37:41+0000","dateFinished":"2019-06-16T14:37:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"%spark\n\n// cell 21\n\n// Here we create a new table that uses all rows from the country table EXCEPT the new test row we inserted\n// Notice that we do not have to explicitly define the columns in the new table.  This is a very convenient way to persist our data for analytics.\n\nvar ds = spark.sql(\"create table countries_2 as select * from countries where country <> 'test_country'\")","user":"anonymous","dateUpdated":"2019-06-16T14:40:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560695927275_-778215195","id":"20190616-143847_815108206","dateCreated":"2019-06-16T14:38:47+0000","dateStarted":"2019-06-16T14:40:07+0000","dateFinished":"2019-06-16T14:40:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"text":"%spark\n\n// cell 22\n\n// Notice that only 124 rows are returned, because our test row was excluded from the new table\n\nvar ds = spark.sql(\"select * from countries_2\")\nds.show\n\nvar ds = spark.sql(\"select count(*) from countries_2\")\nds.show","user":"anonymous","dateUpdated":"2019-06-16T14:44:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560696049395_-204353726","id":"20190616-144049_441394643","dateCreated":"2019-06-16T14:40:49+0000","dateStarted":"2019-06-16T14:44:49+0000","dateFinished":"2019-06-16T14:44:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"text":"%md\n\ncell 23\n\n## Helping Niraj with his Marketing campaign...\n\nSo far we've been experimenting with our SQL capabilities.  Now, let's see how Niraj uses them to accomplish his task.\n\nToday, Niraj wants to select sets of customers who are:\n\n- customers in **specific countries**\n- fairly **active traders**\n- interested in investing in companies that **avoid tobacco products**\n\nHe wants **two tables:** \n\n- **detail-level** information (one row per customer transaction)\n- **aggregate-level** information (one row per customer, in descending order by number of transactions executed)\n\nHe can then use these two tables to do interesting data manipulation in his external tool of choice.\n\nLet's begin with the detail-level record.\n","user":"anonymous","dateUpdated":"2019-06-17T18:08:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560696469195_442193515","id":"20190616-144749_872651136","dateCreated":"2019-06-16T14:47:49+0000","dateStarted":"2019-06-17T18:08:28+0000","dateFinished":"2019-06-17T18:08:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:331","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 23</p>\n<h2>Helping Niraj with his Marketing campaign&hellip;</h2>\n<p>So far we&rsquo;ve been experimenting with our SQL capabilities. Now, let&rsquo;s see how Niraj uses them to accomplish his task.</p>\n<p>Today, Niraj wants to select sets of customers who are:</p>\n<ul>\n  <li>customers in <strong>specific countries</strong></li>\n  <li>fairly <strong>active traders</strong></li>\n  <li>interested in investing in companies that <strong>avoid tobacco products</strong></li>\n</ul>\n<p>He wants <strong>two tables:</strong> </p>\n<ul>\n  <li><strong>detail-level</strong> information (one row per customer transaction)</li>\n  <li><strong>aggregate-level</strong> information (one row per customer, in descending order by number of transactions executed)</li>\n</ul>\n<p>He can then use these two tables to do interesting data manipulation in his external tool of choice.</p>\n<p>Let&rsquo;s begin with the detail-level record.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 24\n\n// detail-level information\n\nvar ds = spark.sql(\"select \"\n                    + \"c.account_number as acct_num, \"\n                    + \"transaction_id, \"\n                    + \"account_country, \"\n                    + \"account_city, \"\n                    + \"account_first_name, \"\n                    + \"account_last_name, \"\n                    + \"avoids_tobacco_min, \"\n                    + \"email, \"\n                    + \"phone, \"\n                    + \"transaction_date, \"\n                    + \"transaction_time, \"\n                    + \"buy_or_sell, \"\n                    + \"units \"\n\n                + \"from \"\n                    + \"transactions t \"\n                    + \"INNER JOIN \"\n                    + \"customers c \"\n        \n                + \"on \"\n                    + \"c.account_number \"\n                    + \"= \"\n                    + \"t.account_number \"\n        \n                + \"where \"\n                    + \"buy_or_sell = 'BUY' \"\n                    + \"AND \"\n                    + \"account_country IN ('Norway', 'Denmark', 'Sweden') \"\n                    + \"AND \"\n                    + \"avoids_tobacco_min > 2 \"\n        \n                + \"order by \"\n                    + \"acct_num asc, \"\n                    + \"transaction_date desc, \"\n                    + \"transaction_time desc\")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-16T18:27:51+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709169276_111907849","id":"20190616-181929_1380794411","dateCreated":"2019-06-16T18:19:29+0000","dateStarted":"2019-06-16T18:27:51+0000","dateFinished":"2019-06-16T18:27:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"%md\n\ncell 25\n\nThe query above gives a current result every time Niraj runs it.\n\nBut now suppose Niraj wants to **persist a snapshot** of his query...\n","user":"anonymous","dateUpdated":"2019-06-17T16:10:54+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709716563_431600247","id":"20190616-182836_684269486","dateCreated":"2019-06-16T18:28:36+0000","dateStarted":"2019-06-17T16:10:54+0000","dateFinished":"2019-06-17T16:10:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:333","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 25</p>\n<p>The query above gives a current result every time Niraj runs it.</p>\n<p>But now suppose Niraj wants to <strong>persist a snapshot</strong> of his query&hellip;</p>\n</div>"}]}},{"text":"%spark\n\n// cell 26\n\n// drop table just in case we have run this notebook before\n\nvar ds = spark.sql(\"drop table if exists campaign_1_detail\")","user":"anonymous","dateUpdated":"2019-06-17T16:20:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560709790472_1133305620","id":"20190616-182950_627980856","dateCreated":"2019-06-16T18:29:50+0000","dateStarted":"2019-06-17T16:20:42+0000","dateFinished":"2019-06-17T16:20:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:334","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nds: org.apache.spark.sql.DataFrame = []\n"}]}},{"text":"%spark\n\n// cell 27\n\n// PERSIST detail-level information \n\nvar ds = spark.sql(\"create table campaign_1_detail as \"\n                    + \"select   \"\n                        + \"c.account_number as acct_num,    \"\n                        + \"transaction_id, \"\n                        + \"account_country, \"\n                        + \"account_city, \"\n                        + \"account_first_name, \"\n                        + \"account_last_name, \"\n                        + \"avoids_tobacco_min, \"\n                        + \"email, \"\n                        + \"phone, \"\n                        + \"transaction_date, \"\n                        + \"transaction_time, \"\n                        + \"buy_or_sell, \"\n                        + \"units \"\n\n                    + \"from \"\n                        + \"transactions t \"\n                        + \"INNER JOIN \"\n                        + \"customers c \"\n        \n                    + \"on \"\n                        + \"c.account_number  \"\n                        + \"= \"\n                        + \"t.account_number \"\n        \n                    + \"where \"\n                        + \"buy_or_sell = 'BUY' \"\n                        + \"AND \"\n                        + \"account_country IN ('Norway', 'Denmark', 'Sweden') \"\n                        + \"AND \"\n                        + \"avoids_tobacco_min > 2 \"\n        \n                    + \"order by \"\n                        + \"acct_num asc, \"\n                        + \"transaction_date desc, \"\n                        + \"transaction_time desc\")","user":"anonymous","dateUpdated":"2019-06-17T16:20:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560787899999_-1230236908","id":"20190617-161139_24113171","dateCreated":"2019-06-17T16:11:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2492","dateFinished":"2019-06-17T16:20:51+0000","dateStarted":"2019-06-17T16:20:47+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nds: org.apache.spark.sql.DataFrame = []\n"}]}},{"text":"%spark\n\n// cell 28\n\n// take a look at the persisted data\n\nvar ds = spark.sql(\"select * from campaign_1_detail\")\nds.show","user":"anonymous","dateUpdated":"2019-06-17T16:22:21+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560788481012_2043065528","id":"20190617-162121_2119500265","dateCreated":"2019-06-17T16:21:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2680","dateFinished":"2019-06-17T16:22:22+0000","dateStarted":"2019-06-17T16:22:21+0000","errorMessage":""},{"text":"%md\n\ncell 29\n\nNow let's see how Niraj builds the **aggregate query**.\n","user":"anonymous","dateUpdated":"2019-06-17T16:23:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560788550586_1079192912","id":"20190617-162230_1590590967","dateCreated":"2019-06-17T16:22:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2767","dateFinished":"2019-06-17T16:23:07+0000","dateStarted":"2019-06-17T16:23:07+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 29</p>\n<p>Now let&rsquo;s see how Niraj builds the <strong>aggregate query</strong>.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 30\n\n// aggregate query\n\nvar ds = spark.sql(\"select  acct_num, \"\n                         + \"count(acct_num) as transaction_count \"\n\n                    + \"from campaign_1_detail \"\n    \n                    + \"group by acct_num \"\n\n                    + \"order by transaction_count desc \")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-17T17:03:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560790723571_-563450692","id":"20190617-165843_1259009569","dateCreated":"2019-06-17T16:58:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2844","dateFinished":"2019-06-17T17:03:19+0000","dateStarted":"2019-06-17T17:03:17+0000","errorMessage":""},{"text":"%md\n\ncell 31\n\nJust as we did with the detail-level query above, let's **persist a snapshot** of the **aggregate query**.","user":"anonymous","dateUpdated":"2019-06-17T17:05:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791116203_758730419","id":"20190617-170516_541505311","dateCreated":"2019-06-17T17:05:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2960","dateFinished":"2019-06-17T17:05:58+0000","dateStarted":"2019-06-17T17:05:58+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 31</p>\n<p>Just as we did with the detail-level query above, let&rsquo;s <strong>persist a snapshot</strong> of the <strong>aggregate query</strong>.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 32\n\n// drop table just in case we have run this notebook before\n\nvar ds = spark.sql(\"drop table if exists campaign_1_aggregate\")","user":"anonymous","dateUpdated":"2019-06-17T17:07:54+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791201886_-384050436","id":"20190617-170641_2087479268","dateCreated":"2019-06-17T17:06:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3083","dateFinished":"2019-06-17T17:07:54+0000","dateStarted":"2019-06-17T17:07:54+0000","errorMessage":""},{"text":"%spark\n\n// cell 33\n\n// aggregate query persisted\n\nvar ds = spark.sql(\"create table campaign_1_aggregate as \"\n\n                    + \"select  acct_num, \"\n                            + \"count(acct_num) as transaction_count \"\n\n                    + \"from campaign_1_detail  \"\n    \n                    + \"group by acct_num \"\n\n                    + \"order by transaction_count desc \")","user":"anonymous","dateUpdated":"2019-06-17T17:12:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791316266_-613609256","id":"20190617-170836_1912322167","dateCreated":"2019-06-17T17:08:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3176","dateFinished":"2019-06-17T17:12:55+0000","dateStarted":"2019-06-17T17:12:53+0000","errorMessage":""},{"text":"%spark\n\n// cell 34\n\n// take a look at the persisted data\n\nvar ds = spark.sql(\"select * from campaign_1_aggregate\")\nds.show","user":"anonymous","dateUpdated":"2019-06-17T17:14:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791602941_669926755","id":"20190617-171322_1551013022","dateCreated":"2019-06-17T17:13:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3267","dateFinished":"2019-06-17T17:14:44+0000","dateStarted":"2019-06-17T17:14:43+0000","errorMessage":""},{"text":"%md\n\ncell 35\n\nNext, let's see how Niraj might **join the two snapshot tables** as part of his campaign data generation.\n","user":"anonymous","dateUpdated":"2019-06-17T17:16:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791749766_535844790","id":"20190617-171549_1964421041","dateCreated":"2019-06-17T17:15:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3381","dateFinished":"2019-06-17T17:16:12+0000","dateStarted":"2019-06-17T17:16:12+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 35</p>\n<p>Next, let&rsquo;s see how Niraj might <strong>join the two snapshot tables</strong> as part of his campaign data generation.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 36\n\n// join snapshots and find high-volume traders\n\nvar ds = spark.sql(\"select distinct \"\n                    + \"a.acct_num, \"\n                    + \"transaction_count, \"\n                    + \"account_country, \"\n                    + \"account_city, \"\n                    + \"account_first_name, \"\n                    + \"account_last_name, \"\n                    + \"email, \"\n                    + \"phone \"\n    \n                + \"from \"\n                    + \"campaign_1_aggregate a \"\n                    + \"LEFT JOIN \"\n                    + \"campaign_1_detail d \"\n    \n                + \"on \"\n                    + \"a.acct_num = d.acct_num \"\n        \n                + \"where \"\n                    + \"transaction_count > 3 \"\n        \n                + \"order by \"\n                    + \"transaction_count desc, \"\n                    + \"account_country asc, \"\n                    + \"account_city asc, \"\n                    + \"account_last_name asc, \"\n                    + \"account_first_name asc \")\nds.show\n","user":"anonymous","dateUpdated":"2019-06-17T17:26:46+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560791823225_1336815379","id":"20190617-171703_282468980","dateCreated":"2019-06-17T17:17:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3458","dateFinished":"2019-06-17T17:26:48+0000","dateStarted":"2019-06-17T17:26:46+0000","errorMessage":""},{"text":"%md\n\ncell 37\n\n## From SQL to Cassandra?\n\nThe work we have done so far is enough to help Niraj persist data **in Spark** for his Marketing campaign.  \n\nBut now let's imagine that Niraj also wanted his **new tables** to appear **as Cassandra tables**... is this happening?\n\nExecute the Cassandra-based cell below to examine the Keyspace.\n","user":"anonymous","dateUpdated":"2019-06-17T17:35:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560792440661_468326553","id":"20190617-172720_1992730577","dateCreated":"2019-06-17T17:27:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3547","dateFinished":"2019-06-17T17:35:57+0000","dateStarted":"2019-06-17T17:35:57+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 37</p>\n<h2>From SQL to Cassandra?</h2>\n<p>The work we have done so far is enough to help Niraj persist data <strong>in Spark</strong> for his Marketing campaign. </p>\n<p>But now let&rsquo;s imagine that Niraj also wanted his <strong>new tables</strong> to appear <strong>as Cassandra tables</strong>&hellip; is this happening?</p>\n<p>Execute the Cassandra-based cell below to examine the Keyspace.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 38\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n// As you will see, the newly-defined tables are not present in Cassandra.\n\ndescribe keyspace analytics_workshop;","user":"anonymous","dateUpdated":"2019-06-17T17:32:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560792648026_248725232","id":"20190617-173048_1418450171","dateCreated":"2019-06-17T17:30:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3651","dateFinished":"2019-06-17T17:32:24+0000","dateStarted":"2019-06-17T17:32:24+0000","errorMessage":""},{"text":"%md\n\ncell 39\n\nNiraj's new tables __do not__ appear in CQL.  That makes sense because CQL table definitions require more information than SQL table definitions.\n\nIn order to make the new tables appear as Cassandra tables, we must define them initially in CQL.  Then they will also appear automatically in the SQL schema as well.\n\nLet's give it a try!  For the sake of simplicity, we'll just implement one of the tables.  Note below that we use CQL, not SQL, to create the table.\n","user":"anonymous","dateUpdated":"2019-06-17T17:36:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560792809026_-1231987226","id":"20190617-173329_462818341","dateCreated":"2019-06-17T17:33:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3733","dateFinished":"2019-06-17T17:36:32+0000","dateStarted":"2019-06-17T17:36:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 39</p>\n<p>Niraj&rsquo;s new tables <strong>do not</strong> appear in CQL. That makes sense because CQL table definitions require more information than SQL table definitions.</p>\n<p>In order to make the new tables appear as Cassandra tables, we must define them initially in CQL. Then they will also appear automatically in the SQL schema as well.</p>\n<p>Let&rsquo;s give it a try! For the sake of simplicity, we&rsquo;ll just implement one of the tables. Note below that we use CQL, not SQL, to create the table.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 40\n\n// create the table using CQL\n\ncreate table if not exists analytics_workshop.campaign_1_aggregate_cassandra (\n    acct_num            int primary key,\n    transaction_count   int\n)\n;\n","user":"anonymous","dateUpdated":"2019-06-17T17:37:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560793029062_1269805008","id":"20190617-173709_712045719","dateCreated":"2019-06-17T17:37:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3938","dateFinished":"2019-06-17T17:37:59+0000","dateStarted":"2019-06-17T17:37:56+0000","errorMessage":""},{"text":"%spark\n\n// cell 41\n\n// RUN THIS ONLY ONCE PER SESSION\n// Just as we did with our other Cassandra tables, we must do a 1-time registration (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW campaign_1_aggregate_cassandra USING org.apache.spark.sql.cassandra OPTIONS ( table 'campaign_1_aggregate_cassandra', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-17T17:57:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560794093054_2076367875","id":"20190617-175453_1853803730","dateCreated":"2019-06-17T17:54:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4134","dateFinished":"2019-06-17T17:57:15+0000","dateStarted":"2019-06-17T17:57:15+0000","errorMessage":""},{"text":"%spark\n\n// cell 42\n\n// now populate the table using Spark SQL\n\nvar ds = spark.sql(\"insert into campaign_1_aggregate_cassandra \"\n                    + \"select  acct_num, \"\n                            + \"count(acct_num) as transaction_count \"\n\n                    + \"from campaign_1_detail  \"\n    \n                    + \"group by acct_num \"\n\n                    + \"order by transaction_count desc \")\n","user":"anonymous","dateUpdated":"2019-06-17T17:57:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560793877037_733454651","id":"20190617-175117_624241579","dateCreated":"2019-06-17T17:51:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4045","dateFinished":"2019-06-17T17:57:37+0000","dateStarted":"2019-06-17T17:57:35+0000","errorMessage":""},{"text":"%cassandra\n\n// cell 43\n\n// read the data using Cassandra CQL\n\nselect * from analytics_workshop.campaign_1_aggregate_cassandra;\n","user":"anonymous","dateUpdated":"2019-06-17T17:58:53+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560794286717_-183972261","id":"20190617-175806_1614629687","dateCreated":"2019-06-17T17:58:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4259","dateFinished":"2019-06-17T17:58:53+0000","dateStarted":"2019-06-17T17:58:53+0000","errorMessage":""},{"text":"%spark\n\n// cell 44\n\n// read the same data using Spark SQL\n\nvar ds = spark.sql(\"select * from campaign_1_aggregate_cassandra\")\nds.show","user":"anonymous","dateUpdated":"2019-06-17T18:04:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560794555873_-1954988417","id":"20190617-180235_123827881","dateCreated":"2019-06-17T18:02:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4390","dateFinished":"2019-06-17T18:04:08+0000","dateStarted":"2019-06-17T18:04:08+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nds: org.apache.spark.sql.DataFrame = [acct_num: int, transaction_count: int]\n+--------+-----------------+\n|acct_num|transaction_count|\n+--------+-----------------+\n|     339|                4|\n|     784|                5|\n|     481|                6|\n|     917|                2|\n|     376|                5|\n|     718|                6|\n|     380|                4|\n|     282|                5|\n|     224|                4|\n|     259|                4|\n|     642|                2|\n|     650|                4|\n|     449|                3|\n|     130|                6|\n|     866|                5|\n|     147|                7|\n|     928|                5|\n|     318|                5|\n|     950|                8|\n+--------+-----------------+\n\n"}]}},{"text":"%md\n\ncell 45\n\n## Summary\n\nIn this lab we examined a wide range of SQL capabilties in Spark SQL, and we showed how they can be used to provide practical information for a business analyst.\n","user":"anonymous","dateUpdated":"2019-06-17T18:05:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560794669443_-480109610","id":"20190617-180429_648624306","dateCreated":"2019-06-17T18:04:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4509","dateFinished":"2019-06-17T18:05:01+0000","dateStarted":"2019-06-17T18:05:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 45</p>\n<h2>Summary</h2>\n<p>In this lab we examined a wide range of SQL capabilties in Spark SQL, and we showed how they can be used to provide practical information for a business analyst.</p>\n</div>"}]}},{"text":"%spark\n","dateUpdated":"2019-06-15T18:48:37+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1560624517330_-827568235","id":"20190530-194003_392853198","dateCreated":"2019-06-15T18:48:37+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:344"}],"name":"Lab_1_-_Spark-SQL_for_Business_Analytics","id":"2EERXCB6B","angularObjects":{"2EG82R1CT:shared_process":[],"2EGDJGTTF:shared_process":[],"2EDDSRFGQ:shared_process":[],"2EDQSPH14:shared_process":[],"2EGANEBGU:shared_process":[],"2EFGW11TW:shared_process":[],"2ECK9UCRJ:shared_process":[],"2ED1GX1AD:shared_process":[],"2EEYRTZAY:shared_process":[],"2EEE1MG63:shared_process":[],"2EFY218XD:shared_process":[],"2EE2ZX7EQ:shared_process":[],"2EDKUCTN1:shared_process":[],"2EEG6D3PT:shared_process":[],"2EEV6VR86:shared_process":[],"2EF1XKYD2:shared_process":[],"2EEDVMK8C:shared_process":[],"2EDVP4F4X:shared_process":[],"2EFSSKAXV:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}