{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 3 - Moving data to and from external systems**\n\nThe modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated.  Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem.  It involves periodic execution of batch processes that move data from one system to another.  Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement ETL processes that do the following:\n\n- **import transaction data** that originated from a business partner\n- **export customer data that** is bound for a data warehouse\n\nWe'll use Spark-SQL to build these processes","dateUpdated":"2019-06-23T19:17:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765123_-1596667139","id":"20190615-193334_2123319074","dateCreated":"2019-06-23T16:19:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8050","user":"anonymous","dateFinished":"2019-06-23T19:17:48+0000","dateStarted":"2019-06-23T19:17:48+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 3 - Moving data to and from external systems</strong></h1>\n<p>The modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated. Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem. It involves periodic execution of batch processes that move data from one system to another. Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement ETL processes that do the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n  <li><strong>export customer data that</strong> is bound for a data warehouse</li>\n</ul>\n<p>We&rsquo;ll use Spark-SQL to build these processes</p>\n</div>"}]}},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine the following tables:\n\n- transactions_buy\n- transactions_sell\n- transactions_historical\n- customers","dateUpdated":"2019-06-23T19:17:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765124_-1598590883","id":"20190615-194453_782238367","dateCreated":"2019-06-23T16:19:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8051","user":"anonymous","dateFinished":"2019-06-23T19:17:53+0000","dateStarted":"2019-06-23T19:17:53+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 2</p>\n<h2>Examining our data&hellip;</h2>\n<p>Execute the cell below to examine the Cassandra Keyspace we will be using in this workshop..</p>\n<p>Take a few minutes to examine the following tables:</p>\n<ul>\n  <li>transactions_buy</li>\n  <li>transactions_sell</li>\n  <li>transactions_historical</li>\n  <li>customers</li>\n</ul>\n</div>"}]}},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n\ndescribe keyspace analytics_workshop;","user":"anonymous","dateUpdated":"2019-06-23T19:06:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765125_-1598975632","id":"20190529-161317_30181527","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:06:58+0000","dateFinished":"2019-06-23T19:06:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8052"},{"text":"%spark\n\n// cell 4\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-23T19:07:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765126_-1597821386","id":"20190615-195433_1606768454","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:07:11+0000","dateFinished":"2019-06-23T19:07:12+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8053"},{"text":"%md\n\ncell 5\n\n### Task 1: ETL **_INTO_** DataStax\n\nWe'll represent external systems by reading some flat files and importing the data into Cassandra tables.\n\nEven though we're using flat files, we'll treat them like database tables.  In real-world implementations, this data might come from a relational database in a legacy system.\n","dateUpdated":"2019-06-23T19:17:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765127_-1598206135","id":"20190620-185404_1847570742","dateCreated":"2019-06-23T16:19:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8054","user":"anonymous","dateFinished":"2019-06-23T19:17:58+0000","dateStarted":"2019-06-23T19:17:58+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5</p>\n<h3>Task 1: ETL <strong><em>INTO</em></strong> DataStax</h3>\n<p>We&rsquo;ll represent external systems by reading some flat files and importing the data into Cassandra tables.</p>\n<p>Even though we&rsquo;re using flat files, we&rsquo;ll treat them like database tables. In real-world implementations, this data might come from a relational database in a legacy system.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 6\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-23T19:07:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765128_-1600129879","id":"20190620-185951_1579318164","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:07:28+0000","dateFinished":"2019-06-23T19:07:29+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8055"},{"text":"%spark\n\n// cell 7\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\n\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");","user":"anonymous","dateUpdated":"2019-06-23T19:07:46+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765129_-1600514628","id":"20190615-184855_149703656","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:07:46+0000","dateFinished":"2019-06-23T19:07:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8056"},{"text":"%spark\n\n// cell 8\n\n// let's see the data from the csv\n\nincoming_buy_transactions.show","user":"anonymous","dateUpdated":"2019-06-23T19:07:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":314,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765130_-1599360381","id":"20190619-184307_1258099386","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:07:52+0000","dateFinished":"2019-06-23T19:07:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8057"},{"text":"%spark\n\n// cell 9\n\n// Nice!  Now let's turn it into a View so we can use SQL against it.\n\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")","user":"anonymous","dateUpdated":"2019-06-23T19:08:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765131_-1599745130","id":"20190620-191918_752301263","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:08:01+0000","dateFinished":"2019-06-23T19:08:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8058"},{"text":"%spark\n\n// cell 10\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\n\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\nnewBuys.show\n\nvar newBuysCount = spark.sql(\"SELECT COUNT(*) FROM new_buys\")\nnewBuysCount.show\n","user":"anonymous","dateUpdated":"2019-06-23T19:08:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765132_-1601668875","id":"20190615-201132_1184275073","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:08:10+0000","dateFinished":"2019-06-23T19:08:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8059"},{"text":"%spark\n\n// cell 11\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\n\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\n\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\nnewSells.show\n\nvar newSellsCount = spark.sql(\"SELECT COUNT(*) FROM new_sells\")\nnewSellsCount.show","user":"anonymous","dateUpdated":"2019-06-23T19:08:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765133_-1602053624","id":"20190616-130108_1826314840","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:08:21+0000","dateFinished":"2019-06-23T19:08:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8060"},{"text":"%md\n\ncell 12\n\n### Why is this a big deal?\n\nAt this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.\n\nHowever, **_I can now treat all these tables as if they belong to a single relational database!_**\n\nLet's prove it...","dateUpdated":"2019-06-23T19:18:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765133_-1602053624","id":"20190620-194111_554239476","dateCreated":"2019-06-23T16:19:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8061","user":"anonymous","dateFinished":"2019-06-23T19:18:10+0000","dateStarted":"2019-06-23T19:18:09+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 12</p>\n<h3>Why is this a big deal?</h3>\n<p>At this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.</p>\n<p>However, <strong><em>I can now treat all these tables as if they belong to a single relational database!</em></strong></p>\n<p>Let&rsquo;s prove it&hellip;</p>\n</div>"}]}},{"text":"%spark\n\n// cell 13\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n\n// IT ALL ACTS AS A SINGLE DATABASE!!!\n\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\")                         \n","user":"anonymous","dateUpdated":"2019-06-23T19:09:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765135_-1601284126","id":"20190620-194829_383892582","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:09:19+0000","dateFinished":"2019-06-23T19:09:25+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8062"},{"text":"%spark\n\n// cell 14\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-23T19:09:34+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765136_-1590895905","id":"20190620-201216_1498917214","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:09:34+0000","dateFinished":"2019-06-23T19:09:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8063"},{"text":"%cassandra\n\n// cell 15\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-23T19:09:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765136_-1590895905","id":"20190620-202419_1221985005","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:09:43+0000","dateFinished":"2019-06-23T19:09:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8064"},{"text":"%cassandra\n\n// cell 16\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-23T19:09:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765137_-1591280654","id":"20190620-202635_2085496124","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:09:53+0000","dateFinished":"2019-06-23T19:09:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8065"},{"text":"%md\n\ncell 17\n\n### Task 2: ETL **_FROM_** DataStax\n\nNow we'll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.\n","dateUpdated":"2019-06-23T19:18:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765138_-1590126408","id":"20190619-210148_86481232","dateCreated":"2019-06-23T16:19:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8066","user":"anonymous","dateFinished":"2019-06-23T19:18:17+0000","dateStarted":"2019-06-23T19:18:17+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 17</p>\n<h3>Task 2: ETL <strong><em>FROM</em></strong> DataStax</h3>\n<p>Now we&rsquo;ll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 18\n\n// Join data from Cassandra tables customers and transactions_historical\n\nvar ds = spark.sql(\n                          \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM transactions_historical t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \")\n                         \nds.show\n","user":"anonymous","dateUpdated":"2019-06-23T19:10:11+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765139_-1590511157","id":"20190620-220241_805016998","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:10:11+0000","dateFinished":"2019-06-23T19:10:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8067"},{"text":"%spark\n\n// cell 19\n\n// Get a row count of the data we want to output\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-23T19:10:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765140_-1592434901","id":"20190620-220557_769467162","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:10:19+0000","dateFinished":"2019-06-23T19:10:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8068"},{"text":"%spark\n\n// cell 20\n\n// Write the result set to a flat file\n// Note: the repartition(1) is required in order to turn this multi-partition dataset into a single dataset.  It's part of the Spark paradigm.\n// The file is written in a directory that is named after my file name below.  In that directory, the actual csv file name begins with \"part-\"\n// These are just Spark idiosyncrasies.\n// Note that the repartition(1) method is the ONLY thing in this entire workshop that requires any understanding of the low-level Spark API.\n\nds.repartition(1).write.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.output_from_dse.csv\");","user":"anonymous","dateUpdated":"2019-06-23T19:10:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765141_-1592819650","id":"20190620-220743_317732750","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:10:53+0000","dateFinished":"2019-06-23T19:10:55+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8069"},{"text":"%md\n\ncell 21\n\nLet's take a look at the file we just wrote.  Enter the following into a terminal:\n\n```\ncd /tmp/datastax-spark-sql-workshop/data/analytics_workshop.output_from_dse.csv\n\nls\n```\n\nThen do a \"more\" on the file shown in the directory.\n","user":"anonymous","dateUpdated":"2019-06-23T19:18:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561317169022_-453732593","id":"20190623-191249_370488975","dateCreated":"2019-06-23T19:12:49+0000","dateStarted":"2019-06-23T19:18:28+0000","dateFinished":"2019-06-23T19:18:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8070","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 21</p>\n<p>Let&rsquo;s take a look at the file we just wrote. Enter the following into a terminal:</p>\n<pre><code>cd /tmp/datastax-spark-sql-workshop/data/analytics_workshop.output_from_dse.csv\n\nls\n</code></pre>\n<p>Then do a &ldquo;more&rdquo; on the file shown in the directory.</p>\n</div>"}]}},{"text":"%md\n\ncell 22\n\n### Summary\n\nWe have seen that DSE's tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.\n\nWe can see data from many disparate sources and treat it all as if it were a single relational database.\n","user":"anonymous","dateUpdated":"2019-06-23T19:18:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765142_-1591665403","id":"20190620-221624_2092246902","dateCreated":"2019-06-23T16:19:25+0000","dateStarted":"2019-06-23T19:18:32+0000","dateFinished":"2019-06-23T19:18:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8071","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 22</p>\n<h3>Summary</h3>\n<p>We have seen that DSE&rsquo;s tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.</p>\n<p>We can see data from many disparate sources and treat it all as if it were a single relational database.</p>\n</div>"}]}},{"text":"%md\n","dateUpdated":"2019-06-23T16:19:25+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561306765143_-1592050152","id":"20190620-223813_1558396189","dateCreated":"2019-06-23T16:19:25+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8072"}],"name":"Lab_3_-_Spark-SQL_for_ETL","id":"2EGVBC73A","angularObjects":{"2EGF2W1QV:shared_process":[],"2EGWMF4AK:shared_process":[],"2EE2MVJ6E:shared_process":[],"2EGK3JYDY:shared_process":[],"2EG4GZ5BY:shared_process":[],"2EH26HPNQ:shared_process":[],"2EE21A3QR:shared_process":[],"2EFGK223E:shared_process":[],"2EDGEB1G1:shared_process":[],"2EF955MZQ:shared_process":[],"2EEPKT46T:shared_process":[],"2EDT73SJ2:shared_process":[],"2EDQR3M3F:shared_process":[],"2EGNH7JFV:shared_process":[],"2EDHX969E:shared_process":[],"2EE2FNGP2:shared_process":[],"2EES4FGA7:shared_process":[],"2EG1BTKZP:shared_process":[],"2EDCS23GB:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}