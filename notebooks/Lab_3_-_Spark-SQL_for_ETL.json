{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 3 - Moving data to and from external systems**\n\nThe modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated.  Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem.  It involves periodic execution of batch processes that move data from one system to another.  Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement ETL process that do the following:\n\n- **import transaction data** that originated from a business partner\n- **export customer data that** is bound for a data warehouse\n\nWe'll use Spark-SQL to build these processes","dateUpdated":"2019-06-20T22:38:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744587_668516522","id":"20190615-193334_2123319074","dateCreated":"2019-06-20T21:55:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2044","user":"anonymous","dateFinished":"2019-06-20T22:38:29+0000","dateStarted":"2019-06-20T22:38:29+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 3 - Moving data to and from external systems</strong></h1>\n<p>The modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated. Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem. It involves periodic execution of batch processes that move data from one system to another. Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement ETL process that do the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n  <li><strong>export customer data that</strong> is bound for a data warehouse</li>\n</ul>\n<p>We&rsquo;ll use Spark-SQL to build these processes</p>\n</div>"}]}},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine the following tables:\n\n- transactions_buy\n- transactions_sell\n- transactions_historical\n- customers","dateUpdated":"2019-06-20T22:38:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744589_666208028","id":"20190615-194453_782238367","dateCreated":"2019-06-20T21:55:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2045","user":"anonymous","dateFinished":"2019-06-20T22:38:32+0000","dateStarted":"2019-06-20T22:38:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 2</p>\n<h2>Examining our data&hellip;</h2>\n<p>Execute the cell below to examine the Cassandra Keyspace we will be using in this workshop..</p>\n<p>Take a few minutes to examine the following tables:</p>\n<ul>\n  <li>transactions_buy</li>\n  <li>transactions_sell</li>\n  <li>transactions_historical</li>\n  <li>customers</li>\n</ul>\n</div>"}]}},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n// For this lab, we are interested in the \"review\" and \"avg_review\" tables\n\ndescribe keyspace analytics_workshop;","user":"anonymous","dateUpdated":"2019-06-20T21:59:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744590_667362275","id":"20190529-161317_30181527","dateCreated":"2019-06-20T21:55:44+0000","dateStarted":"2019-06-20T21:59:57+0000","dateFinished":"2019-06-20T22:00:08+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2046"},{"text":"%spark\n\n// cell 4\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-20T22:00:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744591_666977526","id":"20190615-195433_1606768454","dateCreated":"2019-06-20T21:55:44+0000","dateStarted":"2019-06-20T22:00:19+0000","dateFinished":"2019-06-20T22:00:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2047"},{"text":"%md\n\ncell 5\n\n### Task 1: ETL **_INTO_** DataStax\n\nWe'll represent external systems by reading some flat files and importing the data into Cassandra tables.\n\nEven though we're using flat files, we'll treat them like database tables.  In real-world implementations, this data might come from a relational database in a legacy system.\n","dateUpdated":"2019-06-20T22:38:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744591_666977526","id":"20190620-185404_1847570742","dateCreated":"2019-06-20T21:55:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2048","user":"anonymous","dateFinished":"2019-06-20T22:38:38+0000","dateStarted":"2019-06-20T22:38:38+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5</p>\n<h3>Task 1: ETL <strong><em>INTO</em></strong> DataStax</h3>\n<p>We&rsquo;ll represent external systems by reading some flat files and importing the data into Cassandra tables.</p>\n<p>Even though we&rsquo;re using flat files, we&rsquo;ll treat them like database tables. In real-world implementations, this data might come from a relational database in a legacy system.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 6\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744593_676980998","id":"20190620-185951_1579318164","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2049"},{"text":"%spark\n\n// cell 7\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\n\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744593_676980998","id":"20190615-184855_149703656","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2050"},{"text":"%spark\n\n// cell 8\n\n// let's see the data from the csv\n\nincoming_buy_transactions.show","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":314,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744594_678135244","id":"20190619-184307_1258099386","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2051"},{"text":"%spark\n\n// cell 9\n\n// Nice!  Now let's turn it into a View so we can use SQL against it.\n\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744595_677750495","id":"20190620-191918_752301263","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2052"},{"text":"%spark\n\n// cell 10\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\n\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\nnewBuys.show\n\nvar newBuysCount = spark.sql(\"SELECT COUNT(*) FROM new_buys\")\nnewBuysCount.show\n","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744596_675826751","id":"20190615-201132_1184275073","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2053"},{"text":"%spark\n\n// cell 11\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\n\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\n\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\nnewSells.show\n\nvar newSellsCount = spark.sql(\"SELECT COUNT(*) FROM new_sells\")\nnewSellsCount.show","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744597_675442002","id":"20190616-130108_1826314840","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2054"},{"text":"%md\n\ncell 12\n\n### Why is this a big deal?\n\nAt this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.\n\nHowever, **_I can now treat all these tables as if they belong to a single relational database!_**\n\nLet's prove it...","dateUpdated":"2019-06-20T22:38:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744598_676596249","id":"20190620-194111_554239476","dateCreated":"2019-06-20T21:55:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2055","user":"anonymous","dateFinished":"2019-06-20T22:38:44+0000","dateStarted":"2019-06-20T22:38:44+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 12</p>\n<h3>Why is this a big deal?</h3>\n<p>At this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.</p>\n<p>However, <strong><em>I can now treat all these tables as if they belong to a single relational database!</em></strong></p>\n<p>Let&rsquo;s prove it&hellip;</p>\n</div>"}]}},{"text":"%spark\n\n// cell 13\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n\n// IT ALL ACTS AS A SINGLE DATABASE!!!\n\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\")                         \n","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744599_676211500","id":"20190620-194829_383892582","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2056"},{"text":"%spark\n\n// cell 14\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744600_674287755","id":"20190620-201216_1498917214","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2057"},{"text":"%cassandra\n\n// cell 15\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-20T21:55:44+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744600_674287755","id":"20190620-202419_1221985005","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2058"},{"text":"%cassandra\n\n// cell 16\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-20T21:55:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744601_673903006","id":"20190620-202635_2085496124","dateCreated":"2019-06-20T21:55:44+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2059"},{"text":"%md\n\ncell 17\n\n### Task 2: ETL **_FROM_** DataStax\n\nNow we'll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.\n","user":"anonymous","dateUpdated":"2019-06-20T22:38:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561067744602_675057253","id":"20190619-210148_86481232","dateCreated":"2019-06-20T21:55:44+0000","dateStarted":"2019-06-20T22:38:50+0000","dateFinished":"2019-06-20T22:38:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2060","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 17</p>\n<h3>Task 2: ETL <strong><em>FROM</em></strong> DataStax</h3>\n<p>Now we&rsquo;ll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.</p>\n</div>"}]}},{"text":"%spark\n\n// cell 18\n\n// Join data from Cassandra tables customers and transactions_historical\n\nvar ds = spark.sql(\n                          \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM transactions_historical t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \")\n                         \nds.show\n","user":"anonymous","dateUpdated":"2019-06-20T22:08:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561068161586_1177319845","id":"20190620-220241_805016998","dateCreated":"2019-06-20T22:02:41+0000","dateStarted":"2019-06-20T22:08:49+0000","dateFinished":"2019-06-20T22:08:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2061"},{"text":"%spark\n\n// cell 19\n\n// Get a row count of the data we want to output\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-20T22:36:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561068357898_-44583010","id":"20190620-220557_769467162","dateCreated":"2019-06-20T22:05:57+0000","dateStarted":"2019-06-20T22:36:01+0000","dateFinished":"2019-06-20T22:36:02+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2062"},{"text":"%spark\n\n// cell 20\n\n// Write the result set to a flat file\n// Note: the repartition(1) is required in order to turn this multi-partition dataset into a single dataset.  It's part of the Spark paradigm.\n// The file is written in a directory that is named after my file name below.  In that directory, the actual csv file name begins with \"part-\"\n// These are just Spark idiosyncrasies.\n\nds.repartition(1).write.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.output_from_dse.csv\");","user":"anonymous","dateUpdated":"2019-06-20T22:36:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561068463557_-1445668294","id":"20190620-220743_317732750","dateCreated":"2019-06-20T22:07:43+0000","dateStarted":"2019-06-20T22:36:09+0000","dateFinished":"2019-06-20T22:36:09+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2063"},{"text":"%md\n\ncell 21\n\n### Summary\n\nWe have seen that DSE's tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.\n\nWe can see data from many disparate sources and treat it all as if it were a single relational database.\n","user":"anonymous","dateUpdated":"2019-06-20T22:38:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561068984934_-619479945","id":"20190620-221624_2092246902","dateCreated":"2019-06-20T22:16:24+0000","dateStarted":"2019-06-20T22:38:58+0000","dateFinished":"2019-06-20T22:38:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2064","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 21</p>\n<h3>Summary</h3>\n<p>We have seen that DSE&rsquo;s tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.</p>\n<p>We can see data from many disparate sources and treat it all as if it were a single relational database.</p>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2019-06-20T22:38:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561070293301_1361769749","id":"20190620-223813_1558396189","dateCreated":"2019-06-20T22:38:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2065"}],"name":"Lab_3_-_Spark-SQL_for_ETL","id":"2ED2WCKN7","angularObjects":{"2ECVRZR3Q:shared_process":[],"2EDQ7D4NA:shared_process":[],"2EFJ9PVN2:shared_process":[],"2EE94TV22:shared_process":[],"2EEPDBU3Y:shared_process":[],"2EFP7YY3R:shared_process":[],"2EGR8F82M:shared_process":[],"2EDE5CW47:shared_process":[],"2EEHSMWHA:shared_process":[],"2ED5CP7MR:shared_process":[],"2EEG4BZSC:shared_process":[],"2ED27936N:shared_process":[],"2EEW7SXBN:shared_process":[],"2EE28JE1S:shared_process":[],"2EGK496RD:shared_process":[],"2EFBFU4BM:shared_process":[],"2EF81DE99:shared_process":[],"2EER6XTTD:shared_process":[],"2EGT4QCFT:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}