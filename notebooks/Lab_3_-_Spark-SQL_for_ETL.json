{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 3 - Moving data to and from external systems**\n\nThe modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated.  Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem.  It involves periodic execution of batch processes that move data from one system to another.  Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement ETL processes that do the following:\n\n- **import transaction data** that originated from a business partner\n- **export customer data that** is bound for a data warehouse\n\nWe'll use Spark-SQL to build these processes","user":"anonymous","dateUpdated":"2019-06-21T12:15:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 3 - Moving data to and from external systems</strong></h1>\n<p>The modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated. Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem. It involves periodic execution of batch processes that move data from one system to another. Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement ETL processes that do the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n  <li><strong>export customer data that</strong> is bound for a data warehouse</li>\n</ul>\n<p>We&rsquo;ll use Spark-SQL to build these processes</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553578_-970590561","id":"20190615-193334_2123319074","dateCreated":"2019-06-21T11:29:13+0000","dateStarted":"2019-06-21T12:15:13+0000","dateFinished":"2019-06-21T12:15:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6160"},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine the following tables:\n\n- transactions_buy\n- transactions_sell\n- transactions_historical\n- customers","dateUpdated":"2019-06-21T11:29:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 2</p>\n<h2>Examining our data&hellip;</h2>\n<p>Execute the cell below to examine the Cassandra Keyspace we will be using in this workshop..</p>\n<p>Take a few minutes to examine the following tables:</p>\n<ul>\n  <li>transactions_buy</li>\n  <li>transactions_sell</li>\n  <li>transactions_historical</li>\n  <li>customers</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553580_-972899054","id":"20190615-194453_782238367","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6161"},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n\ndescribe keyspace analytics_workshop;","dateUpdated":"2019-06-21T12:15:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553581_-973283803","id":"20190529-161317_30181527","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6162"},{"text":"%spark\n\n// cell 4\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553582_-972129556","id":"20190615-195433_1606768454","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6163"},{"text":"%md\n\ncell 5\n\n### Task 1: ETL **_INTO_** DataStax\n\nWe'll represent external systems by reading some flat files and importing the data into Cassandra tables.\n\nEven though we're using flat files, we'll treat them like database tables.  In real-world implementations, this data might come from a relational database in a legacy system.\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5</p>\n<h3>Task 1: ETL <strong><em>INTO</em></strong> DataStax</h3>\n<p>We&rsquo;ll represent external systems by reading some flat files and importing the data into Cassandra tables.</p>\n<p>Even though we&rsquo;re using flat files, we&rsquo;ll treat them like database tables. In real-world implementations, this data might come from a relational database in a legacy system.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553583_-972514305","id":"20190620-185404_1847570742","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6164"},{"text":"%cassandra\n\n// cell 6\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553584_-962126085","id":"20190620-185951_1579318164","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6165"},{"text":"%spark\n\n// cell 7\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\n\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553585_-962510834","id":"20190615-184855_149703656","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6166"},{"text":"%spark\n\n// cell 8\n\n// let's see the data from the csv\n\nincoming_buy_transactions.show","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":314,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553585_-962510834","id":"20190619-184307_1258099386","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6167"},{"text":"%spark\n\n// cell 9\n\n// Nice!  Now let's turn it into a View so we can use SQL against it.\n\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553586_-961356587","id":"20190620-191918_752301263","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6168"},{"text":"%spark\n\n// cell 10\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\n\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\nnewBuys.show\n\nvar newBuysCount = spark.sql(\"SELECT COUNT(*) FROM new_buys\")\nnewBuysCount.show\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553587_-961741336","id":"20190615-201132_1184275073","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6169"},{"text":"%spark\n\n// cell 11\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\n\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\n\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\nnewSells.show\n\nvar newSellsCount = spark.sql(\"SELECT COUNT(*) FROM new_sells\")\nnewSellsCount.show","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553587_-961741336","id":"20190616-130108_1826314840","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6170"},{"text":"%md\n\ncell 12\n\n### Why is this a big deal?\n\nAt this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.\n\nHowever, **_I can now treat all these tables as if they belong to a single relational database!_**\n\nLet's prove it...","dateUpdated":"2019-06-21T11:29:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 12</p>\n<h3>Why is this a big deal?</h3>\n<p>At this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.</p>\n<p>However, <strong><em>I can now treat all these tables as if they belong to a single relational database!</em></strong></p>\n<p>Let&rsquo;s prove it&hellip;</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553588_-963665080","id":"20190620-194111_554239476","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6171"},{"text":"%spark\n\n// cell 13\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n\n// IT ALL ACTS AS A SINGLE DATABASE!!!\n\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\")                         \n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553589_-964049829","id":"20190620-194829_383892582","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6172"},{"text":"%spark\n\n// cell 14\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553590_-962895583","id":"20190620-201216_1498917214","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6173"},{"text":"%cassandra\n\n// cell 15\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"text","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553590_-962895583","id":"20190620-202419_1221985005","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6174"},{"text":"%cassandra\n\n// cell 16\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553591_-963280332","id":"20190620-202635_2085496124","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6175"},{"text":"%md\n\ncell 17\n\n### Task 2: ETL **_FROM_** DataStax\n\nNow we'll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 17</p>\n<h3>Task 2: ETL <strong><em>FROM</em></strong> DataStax</h3>\n<p>Now we&rsquo;ll take some data from Cassandra tables and output them to a flat file. Again the flat file is a stand-in for any external system.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553592_-965204076","id":"20190619-210148_86481232","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6176"},{"text":"%spark\n\n// cell 18\n\n// Join data from Cassandra tables customers and transactions_historical\n\nvar ds = spark.sql(\n                          \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM transactions_historical t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \")\n                         \nds.show\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553592_-965204076","id":"20190620-220241_805016998","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6177"},{"text":"%spark\n\n// cell 19\n\n// Get a row count of the data we want to output\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553593_-965588825","id":"20190620-220557_769467162","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6178"},{"text":"%spark\n\n// cell 20\n\n// Write the result set to a flat file\n// Note: the repartition(1) is required in order to turn this multi-partition dataset into a single dataset.  It's part of the Spark paradigm.\n// The file is written in a directory that is named after my file name below.  In that directory, the actual csv file name begins with \"part-\"\n// These are just Spark idiosyncrasies.\n\nds.repartition(1).write.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.output_from_dse.csv\");","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553594_-964434578","id":"20190620-220743_317732750","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6179"},{"text":"%md\n\ncell 21\n\n### Summary\n\nWe have seen that DSE's tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.\n\nWe can see data from many disparate sources and treat it all as if it were a single relational database.\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 21</p>\n<h3>Summary</h3>\n<p>We have seen that DSE&rsquo;s tightly-integrated Spark-SQL capability is a powerful platform for multi-system data sharing.</p>\n<p>We can see data from many disparate sources and treat it all as if it were a single relational database.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1561116553594_-964434578","id":"20190620-221624_2092246902","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6180"},{"text":"%md\n","dateUpdated":"2019-06-21T11:29:13+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561116553595_-964819327","id":"20190620-223813_1558396189","dateCreated":"2019-06-21T11:29:13+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6181"}],"name":"Lab_3_-_Spark-SQL_for_ETL","id":"2EDMMZ1VQ","angularObjects":{"2EEW72JQD:shared_process":[],"2ECWZ8P4Z:shared_process":[],"2EGK1SM5C:shared_process":[],"2EEJ782VX:shared_process":[],"2EE44WEQC:shared_process":[],"2EDPT5ZYR:shared_process":[],"2EFQTZ5CH:shared_process":[],"2EGER4KMH:shared_process":[],"2EFZQQQZ5:shared_process":[],"2EFU7BTNK:shared_process":[],"2EF6XDT2B:shared_process":[],"2EFZYT24A:shared_process":[],"2EGECCVPG:shared_process":[],"2EEMG3RGF:shared_process":[],"2EEH5QHCK:shared_process":[],"2EEKMFEUJ:shared_process":[],"2EDU9B892:shared_process":[],"2EDYDTYKF:shared_process":[],"2EF4RGVUJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}