{"paragraphs":[{"text":"%md\n\ncell 1\n\n# **Lab 3 - Moving data to and from external systems**\n\nThe modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated.  Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem.  It involves periodic execution of batch processes that move data from one system to another.  Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.\n\n### **Persona:** Grace Hopper, Engineering Manager at RightVest\n\nGrace and her team must implement ETL process that do the following:\n\n- **import transaction data** that originated from a business partner\n- **export customer data that** is bound for a data warehouse\n\nWe'll use Spark-SQL to build these processes","user":"anonymous","dateUpdated":"2019-06-20T20:43:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631530_982557662","id":"20190615-193334_2123319074","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T20:43:09+0000","dateFinished":"2019-06-20T20:43:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1643","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 1</p>\n<h1><strong>Lab 3 - Moving data to and from external systems</strong></h1>\n<p>The modern enterprise is a complex ecosystem of applications, often involving siloed data stores that must be integrated. Extract-Transform-Load (ETL) is perhaps the most common approach to this integration problem. It involves periodic execution of batch processes that move data from one system to another. Spark-SQL is a powerful way to do data import/export for DataStax Enterprise.</p>\n<h3><strong>Persona:</strong> Grace Hopper, Engineering Manager at RightVest</h3>\n<p>Grace and her team must implement ETL process that do the following:</p>\n<ul>\n  <li><strong>import transaction data</strong> that originated from a business partner</li>\n  <li><strong>export customer data that</strong> is bound for a data warehouse</li>\n</ul>\n<p>We&rsquo;ll use Spark-SQL to build these processes</p>\n</div>"}]}},{"text":"%md\ncell 2\n\n## Examining our data...\n\nExecute the cell below to examine the Cassandra Keyspace we will be using in this workshop..\n\nTake a few minutes to examine the following tables:\n\n- transactions_buy\n- transactions_sell\n- transactions_historical\n- customers","user":"anonymous","dateUpdated":"2019-06-20T20:43:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631532_980249169","id":"20190615-194453_782238367","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T20:43:17+0000","dateFinished":"2019-06-20T20:43:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1644","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 2</p>\n<h2>Examining our data&hellip;</h2>\n<p>Execute the cell below to examine the Cassandra Keyspace we will be using in this workshop..</p>\n<p>Take a few minutes to examine the following tables:</p>\n<ul>\n  <li>transactions_buy</li>\n  <li>transactions_sell</li>\n  <li>transactions_historical</li>\n  <li>customers</li>\n</ul>\n</div>"}]}},{"text":"%cassandra\n\n// cell 3\n\n// Examine our DSE Keyspace \n// Note that in this cell, we are using Cassandra CQL, not Spark SQL.\n// For this lab, we are interested in the \"review\" and \"avg_review\" tables\n\ndescribe keyspace analytics_workshop;","dateUpdated":"2019-06-20T18:33:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/undefined","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631533_979864420","id":"20190529-161317_30181527","dateCreated":"2019-06-20T18:33:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1645"},{"text":"%spark\n\n// cell 4\n\n// RUN THIS ONLY ONCE PER SESSION\n// IF YOU HAVE ALREADY RUN LAB1 IN THIS SESSION, YOU DO NOT NEED TO RUN THIS CELL\n\n// Before you use Spark SQL queries, you must do a 1-time registration of each table you want to use (there is no IF NOT EXISTS option)\n// You do not need this if you do a dse spark-submit, which pre-registers all tables\n\nvar createDDL = \"CREATE TEMPORARY VIEW customers USING org.apache.spark.sql.cassandra OPTIONS ( table 'customers', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_buy USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_buy', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_historical USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_historical', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)\n\nvar createDDL = \"CREATE TEMPORARY VIEW transactions_sell USING org.apache.spark.sql.cassandra OPTIONS ( table 'transactions_sell', keyspace 'analytics_workshop', cluster 'Cluster 1', pushdown 'true')\"\nspark.sql(createDDL)","user":"anonymous","dateUpdated":"2019-06-20T18:50:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631535_980633918","id":"20190615-195433_1606768454","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T18:50:19+0000","dateFinished":"2019-06-20T18:50:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1646"},{"text":"%md\n\ncell 5\n\n### Task 1: ETL **_INTO_** DataStax\n\nWe'll represent external systems by reading some flat files and importing the data into Cassandra tables.\n\nEven though we're using flat files, we'll treat them like database tables.  In real-world implementations, this data might come from a relational database in a legacy system.\n","user":"anonymous","dateUpdated":"2019-06-20T20:43:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561056844571_-1233483800","id":"20190620-185404_1847570742","dateCreated":"2019-06-20T18:54:04+0000","dateStarted":"2019-06-20T20:43:24+0000","dateFinished":"2019-06-20T20:43:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1647","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 5</p>\n<h3>Task 1: ETL <strong><em>INTO</em></strong> DataStax</h3>\n<p>We&rsquo;ll represent external systems by reading some flat files and importing the data into Cassandra tables.</p>\n<p>Even though we&rsquo;re using flat files, we&rsquo;ll treat them like database tables. In real-world implementations, this data might come from a relational database in a legacy system.</p>\n</div>"}]}},{"text":"%cassandra\n\n// cell 6\n\n// For our destination table, we'll use transactions_historical.  Let's truncate it so we can easily see the results of our import.\n\n TRUNCATE TABLE analytics_workshop.transactions_historical;","user":"anonymous","dateUpdated":"2019-06-20T20:11:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561057191104_1799231793","id":"20190620-185951_1579318164","dateCreated":"2019-06-20T18:59:51+0000","dateStarted":"2019-06-20T20:11:08+0000","dateFinished":"2019-06-20T20:11:08+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1648"},{"text":"%spark\n\n// cell 7\n\n// We'll read in a .csv file into a Spark-SQL object that can be treated just like a database table.\n// In the real world, this could be a flat file or perhaps a database table.\n\nvar incoming_buy_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_buy.csv\");","user":"anonymous","dateUpdated":"2019-06-20T19:15:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631536_991022138","id":"20190615-184855_149703656","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T19:15:45+0000","dateFinished":"2019-06-20T19:15:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1649"},{"text":"%spark\n\n// cell 8\n\n// let's see the data from the csv\n\nincoming_buy_transactions.show","user":"anonymous","dateUpdated":"2019-06-20T19:22:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":314,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631537_990637389","id":"20190619-184307_1258099386","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T19:17:42+0000","dateFinished":"2019-06-20T19:17:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1650"},{"text":"%spark\n\n// cell 9\n\n// Nice!  Now let's turn it into a View so we can use SQL against it.\n\nincoming_buy_transactions.createOrReplaceTempView(\"new_buys\")","user":"anonymous","dateUpdated":"2019-06-20T19:22:10+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561058358254_114251082","id":"20190620-191918_752301263","dateCreated":"2019-06-20T19:19:18+0000","dateStarted":"2019-06-20T19:22:10+0000","dateFinished":"2019-06-20T19:22:10+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1651"},{"text":"%spark\n\n// cell 10\n\n// Next, let's whittle the table down so it only shows large transactions.\n// Notice that we are now working in pure SQL.\n\nvar newBuys = spark.sql(\"SELECT * FROM new_buys WHERE units > 999\")\nnewBuys.show\n\nvar newBuysCount = spark.sql(\"SELECT COUNT(*) FROM new_buys\")\nnewBuysCount.show\n","user":"anonymous","dateUpdated":"2019-06-20T19:32:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631537_990637389","id":"20190615-201132_1184275073","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T19:32:31+0000","dateFinished":"2019-06-20T19:32:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1652"},{"text":"%spark\n\n// cell 11\n\n// The new_buys table looks good.  Now let's do all the same things and make a new_sells table.\n\nvar incoming_sell_transactions = spark.read.option(\"header\", true).csv(\"/tmp/datastax-spark-sql-workshop/data/analytics_workshop.transactions_sell.csv\");\n\nincoming_sell_transactions.createOrReplaceTempView(\"new_sells\")\n\nvar newSells = spark.sql(\"SELECT * FROM new_sells WHERE units > 999\")\nnewSells.show\n\nvar newSellsCount = spark.sql(\"SELECT COUNT(*) FROM new_sells\")\nnewSellsCount.show","user":"anonymous","dateUpdated":"2019-06-20T19:38:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631538_991791636","id":"20190616-130108_1826314840","dateCreated":"2019-06-20T18:33:51+0000","dateStarted":"2019-06-20T19:38:23+0000","dateFinished":"2019-06-20T19:38:25+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1653"},{"text":"%md\n\ncell 12\n\n### Why is this a big deal?\n\nAt this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.\n\nHowever, **_I can now treat all these tables as if they belong to a single relational database!_**\n\nLet's prove it...","user":"anonymous","dateUpdated":"2019-06-20T20:43:36+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561059671833_-707515682","id":"20190620-194111_554239476","dateCreated":"2019-06-20T19:41:11+0000","dateStarted":"2019-06-20T20:43:36+0000","dateFinished":"2019-06-20T20:43:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1654","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>cell 12</p>\n<h3>Why is this a big deal?</h3>\n<p>At this point, I have a set of Cassandra tables, and also two tables that orginated from flat files.</p>\n<p>However, <strong><em>I can now treat all these tables as if they belong to a single relational database!</em></strong></p>\n<p>Let&rsquo;s prove it&hellip;</p>\n</div>"}]}},{"text":"%spark\n\n// cell 13\n\n// Here is a relatively complex query that Joins the (flat file) transaction_buy table with the (Cassandra) customers table, \n// and then Unions it with a Join between the (flat file) transaction_sell table and the (Cassandra) customers table,\n// and then inserts the entire result into the (Cassandra) transactions_historical table.\n\n// IT ALL ACTS AS A SINGLE DATABASE!!!\n\nvar ds = spark.sql(\"INSERT INTO transactions_historical \"\n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_buys t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999 \"\n                         \n                         + \"UNION \"\n                         \n                         + \"SELECT \"\n                             + \"c.account_number        AS account_number, \"\n                             + \"t.transaction_id        AS transaction_id, \"\n                             + \"c.city                  AS account_city, \"\n                             + \"c.country               AS account_country, \"\n                             + \"c.first_name            AS account_first_name, \"\n                             + \"c.last_name             AS account_last_name, \"\n                             + \"c.gender                AS account_gender, \"\n                             + \"t.buy_or_sell           AS buy_or_sell, \"\n                             + \"t.industry_sector       AS industry_sector, \"\n                             + \"t.instrument_id         AS instrument_id, \"\n                             + \"t.instrument_industry   AS instrument_industry, \"\n                             + \"t.instrument_name       AS instrument_name, \"\n                             + \"t.transaction_date      AS transaction_date, \"\n                             + \"t.transaction_time      AS transaction_time, \"\n                             + \"t.units                 AS units \"\n                             \n                         + \"FROM new_sells t INNER JOIN customers c \"\n                         + \"ON t.account_number = c.account_number \"\n                         + \"WHERE t.units > 999\")                         \n","user":"anonymous","dateUpdated":"2019-06-20T20:32:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561060109712_1639713120","id":"20190620-194829_383892582","dateCreated":"2019-06-20T19:48:29+0000","dateStarted":"2019-06-20T20:18:18+0000","dateFinished":"2019-06-20T20:18:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1655"},{"text":"%spark\n\n// cell 14\n\n// Examine the Cassandra output table\n\nvar ds = spark.sql(\"SELECT * FROM transactions_historical\")\nds.show\n\nvar dsCount = spark.sql(\"SELECT COUNT(*) FROM transactions_historical\")\ndsCount.show","user":"anonymous","dateUpdated":"2019-06-20T20:18:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561061536176_2065635609","id":"20190620-201216_1498917214","dateCreated":"2019-06-20T20:12:16+0000","dateStarted":"2019-06-20T20:18:38+0000","dateFinished":"2019-06-20T20:18:39+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1656"},{"text":"%cassandra\n\n// cell 15\n\n// We'll read the same data using CQL, just to prove it really was written to Cassandra\n\nSELECT * FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-20T20:30:00+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561062259182_1298505955","id":"20190620-202419_1221985005","dateCreated":"2019-06-20T20:24:19+0000","dateStarted":"2019-06-20T20:30:00+0000","dateFinished":"2019-06-20T20:30:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1657"},{"text":"%cassandra\n\n// cell 16\n\n// We'll get the same count using CQL, just to prove it really was written to Cassandra\n\nSELECT COUNT(*) FROM analytics_workshop.transactions_historical;\n","user":"anonymous","dateUpdated":"2019-06-20T20:30:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/undefined","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561062395830_-1233932087","id":"20190620-202635_2085496124","dateCreated":"2019-06-20T20:26:35+0000","dateStarted":"2019-06-20T20:30:38+0000","dateFinished":"2019-06-20T20:30:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1658"},{"text":"%md\n","dateUpdated":"2019-06-20T18:33:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1561055631551_986789900","id":"20190619-210148_86481232","dateCreated":"2019-06-20T18:33:51+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1659"}],"name":"Lab_3_-_Spark-SQL_for_ETL","id":"2EEFRTNXN","angularObjects":{"2EDD9RA2X:shared_process":[],"2ECXR2JPE:shared_process":[],"2EG3HSA2A:shared_process":[],"2EEEKR8G4:shared_process":[],"2EDZN1PQM:shared_process":[],"2EEECXAU4:shared_process":[],"2EFS4HWNQ:shared_process":[],"2EFE9FMVP:shared_process":[],"2EF2VNZX4:shared_process":[],"2EEJTFXBE:shared_process":[],"2EGNVB599:shared_process":[],"2ECYMKV81:shared_process":[],"2EDWD6WKH:shared_process":[],"2EEAWBNFD:shared_process":[],"2EFMN88X1:shared_process":[],"2EG61RQR7:shared_process":[],"2EFZ1VWFA:shared_process":[],"2EEH25N65:shared_process":[],"2EDBJ8UA1:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}